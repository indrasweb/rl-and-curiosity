{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6648,
     "status": "ok",
     "timestamp": 1584353288098,
     "user": {
      "displayName": "Harry Songhurst",
      "photoUrl": "",
      "userId": "13125849516639484135"
     },
     "user_tz": 0
    },
    "id": "MiWy-wSlg65h",
    "outputId": "d151a6e4-71ba-48a2-e8a6-b13956a998a7"
   },
   "outputs": [],
   "source": [
    "!pip install stable-baselines==2.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14425,
     "status": "ok",
     "timestamp": 1584353295887,
     "user": {
      "displayName": "Harry Songhurst",
      "photoUrl": "",
      "userId": "13125849516639484135"
     },
     "user_tz": 0
    },
    "id": "dZmBmHSCwNRk",
    "outputId": "2fcf254c-34d2-47f1-9ece-a04645d70939"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch as T\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from stable_baselines.common.misc_util import set_global_seeds\n",
    "from stable_baselines.common.cmd_util import make_atari_env\n",
    "from stable_baselines.common.vec_env import VecFrameStack, VecNormalize\n",
    "from stable_baselines.common.running_mean_std import RunningMeanStd\n",
    "\n",
    "from google.colab import drive\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14419,
     "status": "ok",
     "timestamp": 1584353295890,
     "user": {
      "displayName": "Harry Songhurst",
      "photoUrl": "",
      "userId": "13125849516639484135"
     },
     "user_tz": 0
    },
    "id": "ovKc19B7wQSv",
    "outputId": "9528527d-2ac2-43fd-9feb-30153983ebe3"
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'MontezumaRevengeNoFrameskip-v4'\n",
    "SAVE_PATH = '/content/gdrive/My Drive/A2CM/'\n",
    "\n",
    "TOTAL_FRAMES = 5e7    # 50 million frames\n",
    "ROLLOUT_LENGTH = 128  # transitions in each rollout\n",
    "NENV = 64            # parallel environments, increase to decorrolate batches\n",
    "GAMMA = 0.99        # reward discounting coefficient\n",
    "SEED = 420          # blaze it\n",
    "STEPS_PER_ROLLOUT = ROLLOUT_LENGTH*NENV\n",
    "TOTAL_UPDATES = int(TOTAL_FRAMES // STEPS_PER_ROLLOUT)\n",
    "DEVICE = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "\n",
    "set_global_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33350,
     "status": "ok",
     "timestamp": 1584353314831,
     "user": {
      "displayName": "Harry Songhurst",
      "photoUrl": "",
      "userId": "13125849516639484135"
     },
     "user_tz": 0
    },
    "id": "yl0qbkBdxITN",
    "outputId": "3c0a85a9-0019-413c-cfe7-ab2d2fe2275f"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXGBboSkFQ9u"
   },
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "    def __init__(self, print_rate=250):\n",
    "        self.log = {'ep_r':[], 'ep_l':[], 'loss':[], 'pgloss':[], \n",
    "                    'vloss':[], 'ent':[]}\n",
    "        self.n_ep = 0              # total games/episodes\n",
    "        self.n_update = 1          # total weight updates\n",
    "        self.n_frames = 0          # env steps (total from checkpoint)\n",
    "        self.run_frames = 0        # env steps (for this run)\n",
    "        self.max_rwd = -np.inf     # max rwd out of all games played\n",
    "        self.start_time = time()   # time we started *this* run\n",
    "        self.last_checkpoint = 0   # total_frames at last checkpoint\n",
    "        self.print_rate = print_rate\n",
    "\n",
    "    def eta(self):  # get hh:mm:ss left to train\n",
    "        elapsed_time = time() - self.start_time\n",
    "        frames_left = TOTAL_FRAMES - self.n_frames\n",
    "        sec_per_frame = elapsed_time / self.n_frames\n",
    "        sec_left = int(frames_left * sec_per_frame)\n",
    "        eta_str = str(datetime.timedelta(seconds=sec_left))\n",
    "        return eta_str\n",
    "\n",
    "    def fps(self):  # get frames per second\n",
    "        elapsed_time = time() - self.start_time\n",
    "        fps = int(self.run_frames / elapsed_time)\n",
    "        return fps\n",
    "\n",
    "    def sma(self, x):  # simple moving average\n",
    "        div = 200 if len(x) > 200 else len(x)\n",
    "        return sum(list(zip(*x[-div:]))[-1])/div\n",
    "\n",
    "    def print_log(self):\n",
    "        fps = self.fps()\n",
    "        eta = self.eta()\n",
    "        print('-'*10, self.n_update, '/', TOTAL_UPDATES, '-'*10)\n",
    "        print('Num Games:', self.n_ep)\n",
    "        print('Num Frames:', self.n_frames)\n",
    "        print('FPS:', fps)\n",
    "        print('ETA:', eta)\n",
    "        print('SMA Length:', self.sma(self.log['ep_l']))\n",
    "        print('SMA Reward:', self.sma(self.log['ep_r']))\n",
    "        print('SMA Entropy:', self.sma(self.log['ent']))\n",
    "        print('SMA Loss:', self.sma(self.log['loss']))\n",
    "        print('SMA PG Loss:', self.sma(self.log['pgloss']))\n",
    "        print('SMA V Loss:', self.sma(self.log['vloss']))\n",
    "        print('Max reward:', self.max_rwd)\n",
    "\n",
    "    def record(self, ep, loss, pgloss, vloss, ent):\n",
    "        \n",
    "        self.n_update += 1\n",
    "        self.n_frames += STEPS_PER_ROLLOUT\n",
    "        self.run_frames += STEPS_PER_ROLLOUT\n",
    "        fr = (self.n_frames, self.n_update)\n",
    "\n",
    "        # stats about finished episodes/games\n",
    "        for l, r in zip(ep['l'], ep['r']):\n",
    "            self.log['ep_l'].append(fr+(l,))\n",
    "            self.log['ep_r'].append(fr+(r,))\n",
    "            if r > self.max_rwd: self.max_rwd = r\n",
    "            self.n_ep += 1\n",
    "             \n",
    "        # nn training statistics\n",
    "        self.log['loss'].append(fr+(loss,))\n",
    "        self.log['pgloss'].append(fr+(pgloss,))\n",
    "        self.log['vloss'].append(fr+(vloss,))\n",
    "        self.log['ent'].append(fr+(ent,))\n",
    "        \n",
    "        # print log\n",
    "        if self.n_update % self.print_rate == 0:\n",
    "            self.print_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcl3g06pf2Ml"
   },
   "outputs": [],
   "source": [
    "class AC(nn.Module):\n",
    "  \n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super().__init__()\n",
    "        h, w, c = input_shape\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, 8, 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 4, 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 64, 3, 1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        f = self.conv_size(self.conv, (c,h,w))\n",
    "\n",
    "        self.flat = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(f, 512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.backbone = nn.Sequential(self.conv, self.flat)\n",
    "        self.actor = nn.Linear(512, num_actions)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def conv_size(self, net, in_shape):\n",
    "        x = Variable(T.rand(1, *in_shape))\n",
    "        o = net(x)\n",
    "        b = (-1, o.size(1), o.size(2), o.size(3))\n",
    "        return o.data.view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.backbone(x)\n",
    "        return self.actor(latent), self.critic(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8EEfiG09KHcX"
   },
   "outputs": [],
   "source": [
    "def ob_to_torch(x):\n",
    "    x = np.moveaxis(x, -1, 1)\n",
    "    x = T.from_numpy(x).float()\n",
    "    x = x.to(DEVICE)\n",
    "    return x\n",
    "\n",
    "def rollout_generator(env, policy):\n",
    "\n",
    "    ob = ob_to_torch(env.reset())\n",
    "    mb = {'obs':[], 'act':[], 'rwd':[], 'done':[], 'val':[]}\n",
    "    ep = {'l':[], 'r':[]}  # len & total reward of done eps \n",
    "    \n",
    "    for step in count(1):\n",
    "\n",
    "        # we compute gradients in the train loop\n",
    "        with T.no_grad():\n",
    "            logits, val = policy(ob)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "        \n",
    "        new_ob, rwd, done, info = env.step(action)\n",
    "        \n",
    "        # store transition\n",
    "        mb['obs'].append(ob)\n",
    "        mb['act'].append(action)\n",
    "        mb['rwd'].append(T.from_numpy(rwd))\n",
    "        mb['val'].append(val.view(-1))\n",
    "        mb['done'].append(T.from_numpy(done))\n",
    "        for t in info:\n",
    "            if t.get('episode'):\n",
    "                ep['l'].append(t['episode']['l'])\n",
    "                ep['r'].append(t['episode']['r'])\n",
    "\n",
    "        ob = ob_to_torch(new_ob)\n",
    "\n",
    "        if step % ROLLOUT_LENGTH != 0:\n",
    "            continue\n",
    "\n",
    "        ########################################################\n",
    "        ###         Flatten rollout & assign credit          ###\n",
    "        ########################################################\n",
    "        ### Credit is bootstrapped from our critic (value    ###\n",
    "        ### function) if the episode is not terminal. This   ###\n",
    "        ### bootstrapped credit comes from the estimated     ###\n",
    "        ### value of the *next* state, making it TD(0).      ###\n",
    "        ########################################################\n",
    "\n",
    "        for k in mb.keys():\n",
    "            mb[k] = T.stack(mb[k]).to(DEVICE)\n",
    "\n",
    "        # bootstrap from value function - TD(0)\n",
    "        with T.no_grad():\n",
    "            last_val = policy(ob)[1].view(-1) * ~mb['done'][-1]\n",
    "            mb['rwd'][-1] += GAMMA * last_val\n",
    "\n",
    "        # discount (bootstrapped) rewards\n",
    "        for i in reversed(range(ROLLOUT_LENGTH-1)):\n",
    "            mb['rwd'][i] += GAMMA * mb['rwd'][i+1] * ~mb['done'][i]\n",
    "\n",
    "        # flatten to (NENV*ROLLOUT_LENGTH, ...)\n",
    "        for k in mb.keys():\n",
    "            s = mb[k].size()\n",
    "            mb[k] = mb[k].view(STEPS_PER_ROLLOUT, *s[2:])\n",
    "        \n",
    "        yield mb, ep\n",
    "        mb = {'obs':[], 'act':[], 'rwd':[], 'done':[], 'val':[]}\n",
    "        ep = {'l':[], 'r':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMpNi4g-N5dl"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_id):\n",
    "    checkpoint = {\n",
    "        'env': env,\n",
    "        'ac': ac.state_dict(),\n",
    "        'ac_opt': ac_optimizer.state_dict(),\n",
    "        'logger': logger\n",
    "    }\n",
    "    X = str(checkpoint_id)\n",
    "    T.save(checkpoint, SAVE_PATH+X+'.'+ENV_NAME+'.checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Zs4bNIcO66t"
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(file_name):\n",
    "    checkpoint = T.load(file_name)\n",
    "    \n",
    "    venv = make_atari_env(ENV_NAME, num_env=NENV, seed=SEED)\n",
    "    venv = VecFrameStack(venv, n_stack=4)\n",
    "    env = checkpoint['env']\n",
    "    env.set_venv(venv)\n",
    "    in_dim = env.observation_space.shape\n",
    "    policy_dim = env.action_space.n\n",
    "\n",
    "    ac = AC(in_dim, policy_dim).to(DEVICE)\n",
    "    ac.load_state_dict(checkpoint['ac'])\n",
    "    ac_optimizer = Adam(ac.parameters(), 7e-4, eps=1e-5)\n",
    "    ac_optimizer.load_state_dict(checkpoint['ac_opt'])\n",
    "\n",
    "    logger = checkpoint['logger']\n",
    "\n",
    "    return env, ac, ac_optimizer, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DW4Hen3sO-JO"
   },
   "outputs": [],
   "source": [
    "def new_run():\n",
    "\n",
    "    # setup parallelised gym environments (MaxAndSkip=4 by default)\n",
    "    env = make_atari_env(ENV_NAME, num_env=NENV, seed=SEED)\n",
    "    env._max_episode_steps = 4500*4\n",
    "    env = VecFrameStack(env, n_stack=4)\n",
    "    env = VecNormalize(env, norm_reward=False, clip_reward=1e5, gamma=1.0)\n",
    "    in_dim = env.observation_space.shape\n",
    "    policy_dim = env.action_space.n\n",
    "\n",
    "    # get actor + critic\n",
    "    ac = AC(in_dim, policy_dim).to(DEVICE)\n",
    "    ac_optimizer = Adam(ac.parameters(), 7e-4, eps=1e-5)\n",
    "\n",
    "    logger = Logger(25)\n",
    "\n",
    "    return env, ac, ac_optimizer, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HllpcNHN_eFa"
   },
   "outputs": [],
   "source": [
    "# env, ac, ac_optimizer, logger = new_run()\n",
    "env, ac, ac_optimizer, logger = load_checkpoint(SAVE_PATH+'13115392.MontezumaRevengeNoFrameskip-v4.checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12250110,
     "status": "ok",
     "timestamp": 1583874749680,
     "user": {
      "displayName": "Harry Songhurst",
      "photoUrl": "",
      "userId": "11035621439166996120"
     },
     "user_tz": 0
    },
    "id": "mUEzq88WsM1l",
    "outputId": "785fc531-6dcb-43f4-a35a-1c8d0c8c96b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 1625 / 6103 ----------\n",
      "Num Games: 23974\n",
      "Num Frames: 13303808\n",
      "FPS: 196\n",
      "ETA: 2 days, 3:49:54\n",
      "SMA Length: 552.735\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.890363371372223\n",
      "SMA Loss: -0.028806823389604688\n",
      "SMA PG Loss: 6.133727458745852e-05\n",
      "SMA V Loss: 7.0944583425252e-05\n",
      "Max reward: 400.0\n",
      "---------- 1650 / 6103 ----------\n",
      "Num Games: 24338\n",
      "Num Frames: 13508608\n",
      "FPS: 198\n",
      "ETA: 2 days, 3:06:36\n",
      "SMA Length: 561.08\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903636753559114\n",
      "SMA Loss: -0.0288294530659914\n",
      "SMA PG Loss: 5.168388614492869e-05\n",
      "SMA V Loss: 4.499812293165206e-05\n",
      "Max reward: 400.0\n",
      "---------- 1675 / 6103 ----------\n",
      "Num Games: 24696\n",
      "Num Frames: 13713408\n",
      "FPS: 200\n",
      "ETA: 2 days, 2:23:33\n",
      "SMA Length: 595.38\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903644490242004\n",
      "SMA Loss: -0.02881570881232619\n",
      "SMA PG Loss: 5.671524441368092e-05\n",
      "SMA V Loss: 6.243939315940139e-05\n",
      "Max reward: 400.0\n",
      "---------- 1700 / 6103 ----------\n",
      "Num Games: 25066\n",
      "Num Frames: 13918208\n",
      "FPS: 201\n",
      "ETA: 2 days, 1:40:45\n",
      "SMA Length: 581.205\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.890364629030228\n",
      "SMA Loss: -0.028852608306333422\n",
      "SMA PG Loss: 1.98096026224448e-05\n",
      "SMA V Loss: 6.245528753371587e-05\n",
      "Max reward: 400.0\n",
      "---------- 1725 / 6103 ----------\n",
      "Num Games: 25426\n",
      "Num Frames: 14123008\n",
      "FPS: 203\n",
      "ETA: 2 days, 0:59:07\n",
      "SMA Length: 554.155\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903647196292876\n",
      "SMA Loss: -0.028819125201553105\n",
      "SMA PG Loss: 6.417420758225489e-05\n",
      "SMA V Loss: 4.069404801882115e-05\n",
      "Max reward: 400.0\n",
      "---------- 1750 / 6103 ----------\n",
      "Num Games: 25794\n",
      "Num Frames: 14327808\n",
      "FPS: 205\n",
      "ETA: 2 days, 0:19:26\n",
      "SMA Length: 604.9\n",
      "SMA Reward: 1.0\n",
      "SMA Entropy: 2.8903644776344297\n",
      "SMA Loss: -0.02880341579206288\n",
      "SMA PG Loss: 6.231158924492775e-05\n",
      "SMA V Loss: 7.583335265422875e-05\n",
      "Max reward: 400.0\n",
      "---------- 1775 / 6103 ----------\n",
      "Num Games: 26169\n",
      "Num Frames: 14532608\n",
      "FPS: 206\n",
      "ETA: 1 day, 23:40:54\n",
      "SMA Length: 565.115\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903626346588136\n",
      "SMA Loss: -0.028832252025604247\n",
      "SMA PG Loss: 2.5156851388601354e-05\n",
      "SMA V Loss: 9.243356085910647e-05\n",
      "Max reward: 400.0\n",
      "---------- 1800 / 6103 ----------\n",
      "Num Games: 26526\n",
      "Num Frames: 14737408\n",
      "FPS: 208\n",
      "ETA: 1 day, 23:03:10\n",
      "SMA Length: 583.41\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903614830970765\n",
      "SMA Loss: -0.02881455361843109\n",
      "SMA PG Loss: 4.47732636348519e-05\n",
      "SMA V Loss: 8.857456777938921e-05\n",
      "Max reward: 400.0\n",
      "---------- 1825 / 6103 ----------\n",
      "Num Games: 26893\n",
      "Num Frames: 14942208\n",
      "FPS: 209\n",
      "ETA: 1 day, 22:25:54\n",
      "SMA Length: 574.905\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903607976436616\n",
      "SMA Loss: -0.028834315380081535\n",
      "SMA PG Loss: 2.5002902666528826e-05\n",
      "SMA V Loss: 8.857809627656366e-05\n",
      "Max reward: 400.0\n",
      "---------- 1850 / 6103 ----------\n",
      "Num Games: 27241\n",
      "Num Frames: 15147008\n",
      "FPS: 211\n",
      "ETA: 1 day, 21:49:20\n",
      "SMA Length: 592.215\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903602814674376\n",
      "SMA Loss: -0.02879523139446974\n",
      "SMA PG Loss: 6.50759239965737e-05\n",
      "SMA V Loss: 8.658963109077433e-05\n",
      "Max reward: 400.0\n",
      "---------- 1875 / 6103 ----------\n",
      "Num Games: 27603\n",
      "Num Frames: 15351808\n",
      "FPS: 212\n",
      "ETA: 1 day, 21:13:27\n",
      "SMA Length: 553.97\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903604316711426\n",
      "SMA Loss: -0.028784026168286802\n",
      "SMA PG Loss: 7.743163987697699e-05\n",
      "SMA V Loss: 8.429158716362295e-05\n",
      "Max reward: 400.0\n",
      "---------- 1900 / 6103 ----------\n",
      "Num Games: 27977\n",
      "Num Frames: 15556608\n",
      "FPS: 214\n",
      "ETA: 1 day, 20:38:50\n",
      "SMA Length: 581.69\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.890360720157623\n",
      "SMA Loss: -0.028794850343838333\n",
      "SMA PG Loss: 5.812682037003469e-05\n",
      "SMA V Loss: 0.00010125868278764826\n",
      "Max reward: 400.0\n",
      "---------- 1925 / 6103 ----------\n",
      "Num Games: 28348\n",
      "Num Frames: 15761408\n",
      "FPS: 215\n",
      "ETA: 1 day, 20:04:46\n",
      "SMA Length: 540.625\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903607392311095\n",
      "SMA Loss: -0.028817764529958368\n",
      "SMA PG Loss: 3.524804739072351e-05\n",
      "SMA V Loss: 0.00010118824987356945\n",
      "Max reward: 400.0\n",
      "---------- 1950 / 6103 ----------\n",
      "Num Games: 28706\n",
      "Num Frames: 15966208\n",
      "FPS: 217\n",
      "ETA: 1 day, 19:31:25\n",
      "SMA Length: 592.61\n",
      "SMA Reward: 0.5\n",
      "SMA Entropy: 2.8903611707687378\n",
      "SMA Loss: -0.028838437302038073\n",
      "SMA PG Loss: 1.9951449323798443e-05\n",
      "SMA V Loss: 9.044451079985194e-05\n",
      "Max reward: 400.0\n",
      "---------- 1975 / 6103 ----------\n",
      "Num Games: 29066\n",
      "Num Frames: 16171008\n",
      "FPS: 218\n",
      "ETA: 1 day, 18:58:58\n",
      "SMA Length: 588.095\n",
      "SMA Reward: 0.5\n",
      "SMA Entropy: 2.89036236166954\n",
      "SMA Loss: -0.028803754383698105\n",
      "SMA PG Loss: 4.00637137886406e-05\n",
      "SMA V Loss: 0.00011960957096148306\n",
      "Max reward: 400.0\n",
      "---------- 2000 / 6103 ----------\n",
      "Num Games: 29434\n",
      "Num Frames: 16375808\n",
      "FPS: 220\n",
      "ETA: 1 day, 18:26:43\n",
      "SMA Length: 564.83\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903621459007263\n",
      "SMA Loss: -0.02883869064040482\n",
      "SMA PG Loss: 1.315274472517558e-05\n",
      "SMA V Loss: 0.00010355475055069618\n",
      "Max reward: 400.0\n",
      "---------- 2025 / 6103 ----------\n",
      "Num Games: 29800\n",
      "Num Frames: 16580608\n",
      "FPS: 221\n",
      "ETA: 1 day, 17:54:58\n",
      "SMA Length: 594.91\n",
      "SMA Reward: 0.5\n",
      "SMA Entropy: 2.890362240076065\n",
      "SMA Loss: -0.028789141522720454\n",
      "SMA PG Loss: 6.178694932941653e-05\n",
      "SMA V Loss: 0.00010538656285907777\n",
      "Max reward: 400.0\n",
      "---------- 2050 / 6103 ----------\n",
      "Num Games: 30160\n",
      "Num Frames: 16785408\n",
      "FPS: 222\n",
      "ETA: 1 day, 17:23:48\n",
      "SMA Length: 592.76\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.890362377166748\n",
      "SMA Loss: -0.028814229145646096\n",
      "SMA PG Loss: 3.591161775602814e-05\n",
      "SMA V Loss: 0.00010696474865616423\n",
      "Max reward: 400.0\n",
      "---------- 2075 / 6103 ----------\n",
      "Num Games: 30524\n",
      "Num Frames: 16990208\n",
      "FPS: 224\n",
      "ETA: 1 day, 16:53:19\n",
      "SMA Length: 572.895\n",
      "SMA Reward: 0.5\n",
      "SMA Entropy: 2.8903622031211853\n",
      "SMA Loss: -0.028886640360578895\n",
      "SMA PG Loss: -3.8731378649572437e-05\n",
      "SMA V Loss: 0.00011142483800817615\n",
      "Max reward: 400.0\n",
      "---------- 2100 / 6103 ----------\n",
      "Num Games: 30904\n",
      "Num Frames: 17195008\n",
      "FPS: 225\n",
      "ETA: 1 day, 16:23:30\n",
      "SMA Length: 573.955\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.890361567735672\n",
      "SMA Loss: -0.028815883323550225\n",
      "SMA PG Loss: 4.050524782996945e-05\n",
      "SMA V Loss: 9.445297237135719e-05\n",
      "Max reward: 400.0\n",
      "---------- 2125 / 6103 ----------\n",
      "Num Games: 31269\n",
      "Num Frames: 17399808\n",
      "FPS: 226\n",
      "ETA: 1 day, 15:54:24\n",
      "SMA Length: 566.98\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903611314296724\n",
      "SMA Loss: -0.02882726188749075\n",
      "SMA PG Loss: 2.8617196567211066e-05\n",
      "SMA V Loss: 9.546325899425057e-05\n",
      "Max reward: 400.0\n",
      "---------- 2150 / 6103 ----------\n",
      "Num Games: 31643\n",
      "Num Frames: 17604608\n",
      "FPS: 228\n",
      "ETA: 1 day, 15:25:57\n",
      "SMA Length: 562.43\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.890361123085022\n",
      "SMA Loss: -0.02884838596917689\n",
      "SMA PG Loss: 1.9115869035886136e-05\n",
      "SMA V Loss: 7.221749316045134e-05\n",
      "Max reward: 400.0\n",
      "---------- 2175 / 6103 ----------\n",
      "Num Games: 31997\n",
      "Num Frames: 17809408\n",
      "FPS: 229\n",
      "ETA: 1 day, 14:57:58\n",
      "SMA Length: 590.39\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903619837760925\n",
      "SMA Loss: -0.028859496787190436\n",
      "SMA PG Loss: 3.093696695714243e-05\n",
      "SMA V Loss: 2.637090421664823e-05\n",
      "Max reward: 400.0\n",
      "---------- 2200 / 6103 ----------\n",
      "Num Games: 32387\n",
      "Num Frames: 18014208\n",
      "FPS: 230\n",
      "ETA: 1 day, 14:30:26\n",
      "SMA Length: 557.195\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.890363711118698\n",
      "SMA Loss: -0.02888174389488995\n",
      "SMA PG Loss: 8.653812799366278e-06\n",
      "SMA V Loss: 2.6477509321658488e-05\n",
      "Max reward: 400.0\n",
      "---------- 2225 / 6103 ----------\n",
      "Num Games: 32761\n",
      "Num Frames: 18219008\n",
      "FPS: 232\n",
      "ETA: 1 day, 14:02:58\n",
      "SMA Length: 549.88\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903650999069215\n",
      "SMA Loss: -0.028897979725152255\n",
      "SMA PG Loss: -6.855024978449365e-06\n",
      "SMA V Loss: 2.5051188465639028e-05\n",
      "Max reward: 400.0\n",
      "---------- 2250 / 6103 ----------\n",
      "Num Games: 33130\n",
      "Num Frames: 18423808\n",
      "FPS: 233\n",
      "ETA: 1 day, 13:35:37\n",
      "SMA Length: 560.12\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.890366108417511\n",
      "SMA Loss: -0.02888275901786983\n",
      "SMA PG Loss: 9.12776892846523e-06\n",
      "SMA V Loss: 2.3547038062439183e-05\n",
      "Max reward: 400.0\n",
      "---------- 2275 / 6103 ----------\n",
      "Num Games: 33503\n",
      "Num Frames: 18628608\n",
      "FPS: 234\n",
      "ETA: 1 day, 13:09:07\n",
      "SMA Length: 581.04\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903669917583468\n",
      "SMA Loss: -0.028880235543474554\n",
      "SMA PG Loss: 2.1589653649698447e-05\n",
      "SMA V Loss: 3.687958384297474e-06\n",
      "Max reward: 400.0\n",
      "---------- 2300 / 6103 ----------\n",
      "Num Games: 33865\n",
      "Num Frames: 18833408\n",
      "FPS: 235\n",
      "ETA: 1 day, 12:42:58\n",
      "SMA Length: 586.135\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.890368218421936\n",
      "SMA Loss: -0.028895242847502233\n",
      "SMA PG Loss: -4.6603140958723085e-06\n",
      "SMA V Loss: 2.6197786744717178e-05\n",
      "Max reward: 400.0\n",
      "---------- 2325 / 6103 ----------\n",
      "Num Games: 34214\n",
      "Num Frames: 19038208\n",
      "FPS: 236\n",
      "ETA: 1 day, 12:17:39\n",
      "SMA Length: 588.495\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903687608242037\n",
      "SMA Loss: -0.028862231820821763\n",
      "SMA PG Loss: 2.8678467112968066e-05\n",
      "SMA V Loss: 2.5553064769496814e-05\n",
      "Max reward: 400.0\n",
      "---------- 2350 / 6103 ----------\n",
      "Num Games: 34589\n",
      "Num Frames: 19243008\n",
      "FPS: 238\n",
      "ETA: 1 day, 11:53:15\n",
      "SMA Length: 554.62\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.890368963479996\n",
      "SMA Loss: -0.028883891394361854\n",
      "SMA PG Loss: 7.584210322875151e-06\n",
      "SMA V Loss: 2.4426596973867732e-05\n",
      "Max reward: 400.0\n",
      "---------- 2375 / 6103 ----------\n",
      "Num Games: 34973\n",
      "Num Frames: 19447808\n",
      "FPS: 239\n",
      "ETA: 1 day, 11:29:11\n",
      "SMA Length: 555.405\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.890369031429291\n",
      "SMA Loss: -0.028878183141350745\n",
      "SMA PG Loss: 1.3204564193074474e-05\n",
      "SMA V Loss: 2.4603844014983166e-05\n",
      "Max reward: 400.0\n",
      "---------- 2400 / 6103 ----------\n",
      "Num Games: 35341\n",
      "Num Frames: 19652608\n",
      "FPS: 240\n",
      "ETA: 1 day, 11:05:30\n",
      "SMA Length: 565.775\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.89036887049675\n",
      "SMA Loss: -0.028815428484231232\n",
      "SMA PG Loss: 6.438967216126912e-05\n",
      "SMA V Loss: 4.773966703105259e-05\n",
      "Max reward: 400.0\n",
      "---------- 2425 / 6103 ----------\n",
      "Num Games: 35699\n",
      "Num Frames: 19857408\n",
      "FPS: 241\n",
      "ETA: 1 day, 10:42:04\n",
      "SMA Length: 573.67\n",
      "SMA Reward: 0.5\n",
      "SMA Entropy: 2.8903682219982145\n",
      "SMA Loss: -0.028842982472851872\n",
      "SMA PG Loss: 2.5017141342686956e-05\n",
      "SMA V Loss: 7.13638548884532e-05\n",
      "Max reward: 400.0\n",
      "---------- 2450 / 6103 ----------\n",
      "Num Games: 36080\n",
      "Num Frames: 20062208\n",
      "FPS: 242\n",
      "ETA: 1 day, 10:19:01\n",
      "SMA Length: 557.14\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903671276569365\n",
      "SMA Loss: -0.028816734766587616\n",
      "SMA PG Loss: 5.120350473589497e-05\n",
      "SMA V Loss: 7.146478286955471e-05\n",
      "Max reward: 400.0\n",
      "---------- 2475 / 6103 ----------\n",
      "Num Games: 36447\n",
      "Num Frames: 20267008\n",
      "FPS: 243\n",
      "ETA: 1 day, 9:56:13\n",
      "SMA Length: 549.775\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903661811351777\n",
      "SMA Loss: -0.028819853682070972\n",
      "SMA PG Loss: 4.805191798823216e-05\n",
      "SMA V Loss: 7.151118505266396e-05\n",
      "Max reward: 400.0\n",
      "---------- 2500 / 6103 ----------\n",
      "Num Games: 36824\n",
      "Num Frames: 20471808\n",
      "FPS: 244\n",
      "ETA: 1 day, 9:33:43\n",
      "SMA Length: 572.465\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903655517101288\n",
      "SMA Loss: -0.028843420427292585\n",
      "SMA PG Loss: 3.4650943816814106e-05\n",
      "SMA V Loss: 5.116703962443125e-05\n",
      "Max reward: 400.0\n",
      "---------- 2525 / 6103 ----------\n",
      "Num Games: 37181\n",
      "Num Frames: 20676608\n",
      "FPS: 245\n",
      "ETA: 1 day, 9:11:28\n",
      "SMA Length: 595.335\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903655469417573\n",
      "SMA Loss: -0.02884629867039621\n",
      "SMA PG Loss: 3.187719661582378e-05\n",
      "SMA V Loss: 5.095791887690382e-05\n",
      "Max reward: 400.0\n",
      "---------- 2550 / 6103 ----------\n",
      "Num Games: 37554\n",
      "Num Frames: 20881408\n",
      "FPS: 246\n",
      "ETA: 1 day, 8:49:28\n",
      "SMA Length: 573.37\n",
      "SMA Reward: 0.5\n",
      "SMA Entropy: 2.89036523938179\n",
      "SMA Loss: -0.02884916898794472\n",
      "SMA PG Loss: 2.4195877904276132e-05\n",
      "SMA V Loss: 6.0573778193493414e-05\n",
      "Max reward: 400.0\n",
      "---------- 2575 / 6103 ----------\n",
      "Num Games: 37930\n",
      "Num Frames: 21086208\n",
      "FPS: 247\n",
      "ETA: 1 day, 8:27:25\n",
      "SMA Length: 560.97\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903643667697905\n",
      "SMA Loss: -0.028814331283792852\n",
      "SMA PG Loss: 5.366911850614997e-05\n",
      "SMA V Loss: 7.12851880491172e-05\n",
      "Max reward: 400.0\n",
      "---------- 2600 / 6103 ----------\n",
      "Num Games: 38306\n",
      "Num Frames: 21291008\n",
      "FPS: 248\n",
      "ETA: 1 day, 8:05:28\n",
      "SMA Length: 540.18\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.8903635704517363\n",
      "SMA Loss: -0.028912779334932567\n",
      "SMA PG Loss: -3.3379136029907384e-05\n",
      "SMA V Loss: 4.8469668719874905e-05\n",
      "Max reward: 400.0\n",
      "---------- 2625 / 6103 ----------\n",
      "Num Games: 38670\n",
      "Num Frames: 21495808\n",
      "FPS: 249\n",
      "ETA: 1 day, 7:43:28\n",
      "SMA Length: 564.11\n",
      "SMA Reward: 0.0\n",
      "SMA Entropy: 2.890363517999649\n",
      "SMA Loss: -0.028876580558717252\n",
      "SMA PG Loss: 1.4530405187542784e-05\n",
      "SMA V Loss: 2.5047040556742673e-05\n",
      "Max reward: 400.0\n",
      "---------- 2650 / 6103 ----------\n",
      "Num Games: 39039\n",
      "Num Frames: 21700608\n",
      "FPS: 250\n",
      "ETA: 1 day, 7:21:37\n",
      "SMA Length: 569.12\n",
      "SMA Reward: 0.5\n",
      "SMA Entropy: 2.890363862514496\n",
      "SMA Loss: -0.028905773535370827\n",
      "SMA PG Loss: -2.1041461423010333e-05\n",
      "SMA V Loss: 3.7811726129888254e-05\n",
      "Max reward: 400.0\n"
     ]
    }
   ],
   "source": [
    "rollouts = rollout_generator(env, ac)\n",
    "scheduler = LambdaLR(ac_optimizer, lambda i: 1 - i / TOTAL_UPDATES)\n",
    "\n",
    "for i_update in range(logger.n_update, TOTAL_UPDATES):\n",
    "\n",
    "    mb, ep = next(rollouts)\n",
    "\n",
    "    # calculate advantange\n",
    "    logits, val = ac(mb['obs'])\n",
    "    dist = Categorical(logits=logits)\n",
    "    ent = dist.entropy().mean()\n",
    "    logp = dist.log_prob(mb['act'])\n",
    "    adv = mb['rwd'] - mb['val']\n",
    "    \n",
    "    # calculate losses (soft ac with entropy)\n",
    "    vloss = (val.view(-1) - mb['rwd']).pow(2).mean()\n",
    "    pgloss = -(adv * logp).mean()\n",
    "    loss = pgloss + 0.5*vloss - 0.01*ent\n",
    "    \n",
    "    # update policy\n",
    "    ac_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(ac.parameters(), 0.5)\n",
    "    ac_optimizer.step()\n",
    "    scheduler.step(i_update)\n",
    "\n",
    "    logger.record(ep, loss.item(), pgloss.item(), vloss.item(), ent.item())\n",
    "    \n",
    "    # make a checkpoint every 1m frames\n",
    "    if logger.n_frames - logger.last_checkpoint > 1e6:\n",
    "        save_checkpoint(logger.n_frames)\n",
    "        logger.last_checkpoint = logger.n_frames\n",
    "\n",
    "save_checkpoint('FINAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50KIcAvCz58G"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWvCD53L3_j4"
   },
   "outputs": [],
   "source": [
    "def plot_log(log, ylabel):\n",
    "    # compute rolling avg and std\n",
    "    df = pd.DataFrame(log, columns =['Frames', 'Iters', ylabel])\n",
    "    sma_y = df[ylabel].rolling(500).mean()\n",
    "    std_y = df[ylabel].rolling(500).std()\n",
    "    \n",
    "    # plot with seaborn\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.set_xlabel('Frames')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    clrs = sns.color_palette(\"husl\", 8)\n",
    "\n",
    "    # fill standard deviation\n",
    "    ax.plot(df['Frames'], sma_y, label=ylabel, c=clrs[0])\n",
    "    ax.fill_between(df['Frames'], sma_y-std_y,  sma_y+std_y, \n",
    "                    alpha=0.3, facecolor=clrs[0]) \n",
    "    ax.legend(loc='upper left')\n",
    "    plt.savefig(ENV_NAME+'.'+ylabel+'.plt.png', \n",
    "                dpi=300, pad_inches=0)\n",
    "    plt.show()\n",
    "\n",
    "plot_log(logger.log['ep_r'], 'Reward')\n",
    "plot_log(logger.log['ep_l'], 'Length')\n",
    "plot_log(logger.log['loss'], 'Loss')\n",
    "plot_log(logger.log['ent'], 'Entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ws_D64A_Mk6l"
   },
   "outputs": [],
   "source": [
    "fig_all, ax_all = plt.subplots(2,2, figsize=(8,8))\n",
    "\n",
    "## REWARD \n",
    "df = pd.DataFrame(logger.log['ep_r'], columns =['Frames', 'Iters', 'rwd'])\n",
    "sma_y = df['rwd'].rolling(500).mean()\n",
    "std_y = df['rwd'].rolling(500).std()\n",
    "\n",
    "ax_all[0][0].set_ylabel('Reward')\n",
    "clrs = sns.color_palette(\"husl\", 8)\n",
    "\n",
    "# fill standard deviation\n",
    "ax_all[0][0].plot(df['Frames'], sma_y, label='Episode Reward', c=clrs[0])\n",
    "ax_all[0][0].fill_between(df['Frames'], sma_y-std_y,  sma_y+std_y, \n",
    "                alpha=0.3, facecolor=clrs[0])\n",
    "\n",
    "# make x axis nice\n",
    "ax_all[0][0].set_xticklabels('')\n",
    "\n",
    "\n",
    "## LENGTH \n",
    "df = pd.DataFrame(logger.log['ep_l'], columns =['Frames', 'Iters', 'Len'])\n",
    "sma_y = df['Len'].rolling(500).mean()\n",
    "std_y = df['Len'].rolling(500).std()\n",
    "\n",
    "ax_all[0][1].set_ylabel('Length')\n",
    "ax_all[0][1].yaxis.tick_right()\n",
    "clrs = sns.color_palette(\"husl\", 8)\n",
    "\n",
    "# fill standard deviation\n",
    "ax_all[0][1].plot(df['Frames'], sma_y, label='Episode Length', c=clrs[0])\n",
    "ax_all[0][1].fill_between(df['Frames'], sma_y-std_y,  sma_y+std_y, \n",
    "                alpha=0.3, facecolor=clrs[0])\n",
    "\n",
    "# make x axis nice\n",
    "ax_all[0][1].set_xticklabels('')\n",
    "\n",
    "\n",
    "## ENTROPY \n",
    "df = pd.DataFrame(logger.log['ent'], columns =['Frames', 'Iters', 'ent'])\n",
    "sma_y = df['ent'].rolling(500).mean()\n",
    "std_y = df['ent'].rolling(500).std()\n",
    "\n",
    "ax_all[1][0].set_xlabel('Frames')\n",
    "ax_all[1][0].set_ylabel('Entropy')\n",
    "clrs = sns.color_palette(\"husl\", 8)\n",
    "\n",
    "# fill standard deviation\n",
    "ax_all[1][0].plot(df['Frames'], sma_y, label='Entropy', c=clrs[0])\n",
    "ax_all[1][0].fill_between(df['Frames'], sma_y-std_y,  sma_y+std_y, \n",
    "                alpha=0.3, facecolor=clrs[0])\n",
    "\n",
    "# make x axis nice\n",
    "xlabels = [f'{int(x)}M' for x in ax_all[1][0].get_xticks()/1e6]\n",
    "ax_all[1][0].set_xticklabels(xlabels)\n",
    "\n",
    "\n",
    "## LOSS \n",
    "df = pd.DataFrame(logger.log['loss'], columns =['Frames', 'Iters', 'Loss'])\n",
    "sma_y = df['Loss'].rolling(500).mean()\n",
    "std_y = df['Loss'].rolling(500).std()\n",
    "\n",
    "ax_all[1][1].set_xlabel('Frames')\n",
    "ax_all[1][1].set_ylabel('Loss')\n",
    "ax_all[1][1].yaxis.tick_right()\n",
    "clrs = sns.color_palette(\"husl\", 8)\n",
    "\n",
    "# fill standard deviation\n",
    "ax_all[1][1].plot(df['Frames'], sma_y, label='Loss', c=clrs[0])\n",
    "ax_all[1][1].fill_between(df['Frames'], sma_y-std_y,  sma_y+std_y, \n",
    "                alpha=0.3, facecolor=clrs[0])\n",
    "\n",
    "# make x axis nice\n",
    "xlabels = [f'{int(x)}M' for x in ax_all[1][1].get_xticks()/1e6]\n",
    "ax_all[1][1].set_xticklabels(xlabels)\n",
    "\n",
    "plt.savefig(SAVE_PATH+ENV_NAME+'_plot_thesis.png', \n",
    "            dpi=300, pad_inches=0, bbox_inches = 'tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6FH73w2tddEK"
   },
   "outputs": [],
   "source": [
    "# Save log for comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuB1XN55zVpJ"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "log = T.load('FINAL.'+ENV_NAME+'.checkpoint', map_location=T.device('cpu'))['logger'].log\n",
    "pickle.dump(log, open('FINAL.'+ENV_NAME+'.log', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "A2C_Atari_Breakout_128.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
