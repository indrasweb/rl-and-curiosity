{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6169,
     "status": "ok",
     "timestamp": 1582364633813,
     "user": {
      "displayName": "harry songhurst",
      "photoUrl": "",
      "userId": "00629498674930426914"
     },
     "user_tz": 0
    },
    "id": "MiWy-wSlg65h",
    "outputId": "d809c947-cdc1-441c-ac8a-b8ea830acbed"
   },
   "outputs": [],
   "source": [
    "!pip install stable-baselines==2.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14099,
     "status": "ok",
     "timestamp": 1582364641764,
     "user": {
      "displayName": "harry songhurst",
      "photoUrl": "",
      "userId": "00629498674930426914"
     },
     "user_tz": 0
    },
    "id": "dZmBmHSCwNRk",
    "outputId": "6ebbaa1f-b40d-4606-d340-f7ebe4d5fa93"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from stable_baselines.common.misc_util import set_global_seeds\n",
    "from stable_baselines.common.cmd_util import make_atari_env\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14083,
     "status": "ok",
     "timestamp": 1582364641765,
     "user": {
      "displayName": "harry songhurst",
      "photoUrl": "",
      "userId": "00629498674930426914"
     },
     "user_tz": 0
    },
    "id": "ovKc19B7wQSv",
    "outputId": "5be272d6-c3d0-4c4c-9533-b1b4bfb2bae9"
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'BreakoutNoFrameskip-v4'\n",
    "NENV = 8\n",
    "ROLLOUT_LENGTH = 128\n",
    "LATENT_SIZE = 256\n",
    "MB_SPLIT = 4\n",
    "MB_EPOCHS = 4\n",
    "TOTAL_TIMESTEPS = 5e6\n",
    "GAMMA = 0.99\n",
    "CLIP = 0.2\n",
    "DEVICE = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "SEED = 420 \n",
    "set_global_seeds(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNveivUPBrGP"
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17167,
     "status": "ok",
     "timestamp": 1582364644866,
     "user": {
      "displayName": "harry songhurst",
      "photoUrl": "",
      "userId": "00629498674930426914"
     },
     "user_tz": 0
    },
    "id": "DqEfdOszQ9kw",
    "outputId": "509128ae-2500-4fea-f227-09c737feacd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape:  (4, 84, 84)\n",
      "Number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "env = make_atari_env(ENV_NAME, num_env=NENV, seed=SEED)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "in_dim = env.observation_space.shape\n",
    "in_dim=(in_dim[2],in_dim[0],in_dim[1])  # torch ordering\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print('Observation shape: ', in_dim)\n",
    "print('Number of actions: ', num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0VaLTiryZut"
   },
   "source": [
    "# CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBln27mj6Yw4"
   },
   "outputs": [],
   "source": [
    "def conv_size(net, in_shape):\n",
    "    \"\"\" util for calculating flat output shape of a given net \"\"\"\n",
    "    x = Variable(T.rand(1, *in_shape))\n",
    "    o = net(x)\n",
    "    b = (-1, o.size(1), o.size(2), o.size(3))\n",
    "    return b, o.data.view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxKbJpAFMYNk"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \"\"\" This is used to *represent* the environment (perception).\n",
    "        The latent space of this autoencoder is uni-dimensional.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, num_actions, latent_size):\n",
    "        super().__init__()\n",
    "        self.c, self.h, self.w = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        self.encoder_conv = nn.Sequential(\n",
    "        nn.Conv2d(self.c, 64, 8, 4),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(64, 32, 4, 2),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(32, 16, 4, 1),\n",
    "        nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.conv_x, self.conv_f = conv_size(self.encoder_conv, input_shape)\n",
    "\n",
    "        self.encoder_linear = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(self.conv_f, self.latent_size)\n",
    "        )\n",
    "\n",
    "        self.decoder_linear = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.conv_f),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "        nn.ConvTranspose2d(16+self.num_actions, 32, 4, 1),  # note num_actions\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(32, 64, 4, 2),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(64, self.c, 8, 4)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\" *represent* the environment in latent space\"\"\"\n",
    "        x = self.encoder_conv(x)\n",
    "        x = self.encoder_linear(x)\n",
    "        return x  # latent code\n",
    "\n",
    "    def one_hot_3d_action(self, actions):\n",
    "        _,_,ch,cw = self.conv_x\n",
    "        cn = len(actions)\n",
    "        z = np.zeros((len(actions), self.num_actions, ch, cw), dtype=np.float32)\n",
    "        for i in range(cn):\n",
    "            z[i,actions[i],:,:] = 1.0\n",
    "        z = T.from_numpy(z).to(DEVICE)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x, actions):\n",
    "        \"\"\" actions is a list of ints describing the actions taken \"\"\"\n",
    "        x = self.encoder_conv(x)\n",
    "        latent = self.encoder_linear(x)\n",
    "        x = self.decoder_linear(latent)\n",
    "        x = x.view(self.conv_x)\n",
    "        # add information about which action we took\n",
    "        z_action = self.one_hot_3d_action(actions)\n",
    "        x = T.cat((x, z_action), dim=1)\n",
    "        x = self.decoder_conv(x)\n",
    "        return x, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kx6vqbdxSSMZ"
   },
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-75xf3rLSAGt"
   },
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\" This is used to *represent* the environment (perception).\n",
    "        The latent space of this autoencoder is uni-dimensional.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, num_actions, latent_size):\n",
    "        super().__init__()\n",
    "        self.c, self.h, self.w = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        self.encoder_conv = nn.Sequential(\n",
    "        nn.Conv2d(self.c, 64, 8, 4),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(64, 32, 4, 2),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(32, 16, 4, 1),\n",
    "        nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.conv_x, self.conv_f = conv_size(self.encoder_conv, input_shape)\n",
    "\n",
    "        self.encoder_linear = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(self.conv_f, self.latent_size)\n",
    "        )\n",
    "\n",
    "        self.decoder_linear = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.conv_f),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "        nn.ConvTranspose2d(16+self.num_actions, 32, 4, 1),  # note num_actions\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(32, 64, 4, 2),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(64, self.c, 8, 4)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\" *represent* the environment in latent space\"\"\"\n",
    "        x = self.encoder_conv(x)\n",
    "        x = self.encoder_linear(x)\n",
    "        return x  # latent code\n",
    "\n",
    "    def one_hot_3d_action(self, actions):\n",
    "        _,_,ch,cw = self.conv_x\n",
    "        cn = len(actions)\n",
    "        z = np.zeros((len(actions), self.num_actions, ch, cw), dtype=np.float32)\n",
    "        for i in range(cn):\n",
    "            z[i,actions[i],:,:] = 1.0\n",
    "        z = T.from_numpy(z).to(DEVICE)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x, actions):\n",
    "        \"\"\" actions is a list of ints describing the actions taken \"\"\"\n",
    "        x = self.encoder_conv(x)\n",
    "        latent = self.encoder_linear(x)\n",
    "        x = self.decoder_linear(latent)\n",
    "        x = x.view(self.conv_x)\n",
    "        # add information about which action we took\n",
    "        z_action = self.one_hot_3d_action(actions)\n",
    "        x = T.cat((x, z_action), dim=1)\n",
    "        x = self.decoder_conv(x)\n",
    "        return x, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3nHtIUDs66u"
   },
   "outputs": [],
   "source": [
    "class ACC(nn.Module):\n",
    "    \"\"\" This is used to learn *behaviour* from our environment representation.\n",
    "        ActorCriticCritic uses two value heads/critics. One criticises the \n",
    "        extrinsic reward. The other criticises the intrinsic reward.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Linear(512, num_actions)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "        # self.int_critic = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return self.actor(x), self.critic(x)  #, self.int_critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LTuW2HWsuFs6"
   },
   "outputs": [],
   "source": [
    "class ACTOR(nn.Module):\n",
    "    \"\"\" This is used to learn *behaviour* from our environment representation.\n",
    "        ActorCriticCritic uses two value heads/critics. One criticises the \n",
    "        extrinsic reward. The other criticises the intrinsic reward.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return self.actor(x)\n",
    "\n",
    "class CRITIC(nn.Module):\n",
    "    \"\"\" This is used to learn *behaviour* from our environment representation.\n",
    "        ActorCriticCritic uses two value heads/critics. One criticises the \n",
    "        extrinsic reward. The other criticises the intrinsic reward.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return self.critic(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fsXOZOh64ciH"
   },
   "source": [
    "# Rollout handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1gk4kwcR-yy"
   },
   "outputs": [],
   "source": [
    "class Rollouts:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.empty_memory()\n",
    "\n",
    "    def store_transition(self, ob, logp, action, reward, value, done, info):\n",
    "        self.obs.append(ob)\n",
    "        self.logps.append(logp)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "        for i in info:\n",
    "            if i.get('episode'): \n",
    "                self.infos['l'].append(i['episode']['l'])\n",
    "                self.infos['r'].append(i['episode']['r'])\n",
    "\n",
    "    def empty_memory(self):\n",
    "        self.obs      = []\n",
    "        self.obs_d1   = []  # t+1\n",
    "        self.obs_d2   = []  # t+2\n",
    "        self.logps    = []\n",
    "        self.actions  = []\n",
    "        self.rewards  = []\n",
    "        self.values   = []\n",
    "        self.dones    = []\n",
    "        self.infos   = {'l':[], 'r':[]}\n",
    "\n",
    "    def preprocess_obs(self, arr):\n",
    "        \"\"\" Normalize and permute array :param arr: to make it Torch compatible \"\"\"\n",
    "        return np.moveaxis(arr.astype(np.float32) / 255., -1, 1)\n",
    "\n",
    "    def flatten(self, x, flat_n, to_tensor=True):\n",
    "        \"\"\" flatten x *in place* to (flat_n, ...)\n",
    "            :param to_tensor: if true, x is cast to a torch tenor \"\"\"\n",
    "        x = np.asarray(x, np.float32).reshape((flat_n,)+x[0].shape[1:])\n",
    "        if to_tensor: \n",
    "            return T.from_numpy(x).to(DEVICE)\n",
    "        return x\n",
    "\n",
    "    def flatten_rollout(self):\n",
    "        \"\"\" flatten trajectory buffers to (NENV*ROLLOUT_LENGTH-1, ...)\"\"\"\n",
    "        N = (ROLLOUT_LENGTH-1)*NENV\n",
    "        self.obs     = self.flatten(self.obs, N)\n",
    "        self.obs_d1  = self.flatten(self.obs_d1, N)\n",
    "        self.obs_d2  = self.flatten(self.obs_d2, N)\n",
    "        self.logps   = self.flatten(self.logps, N)\n",
    "        self.actions = self.flatten(self.actions, N)\n",
    "        self.rewards = self.flatten(self.rewards, N)\n",
    "        self.values  = self.flatten(self.values, N)\n",
    "\n",
    "\n",
    "    def generator(self):\n",
    "\n",
    "        ob = self.preprocess_obs(env.reset())  # initial observation\n",
    "        rollout_num = 0\n",
    "\n",
    "        while True:\n",
    "            rollout_num += 1\n",
    "            \n",
    "            with T.no_grad():\n",
    "                tob = T.from_numpy(ob).to(DEVICE)\n",
    "                z1 = ae_delta_1.encode(tob)      # latent repr of St+1\n",
    "                z2 = ae_delta_2.encode(tob)      # latent repr of St+2\n",
    "                z_join = T.cat((z1,z2), dim=1)   # joint latent repr\n",
    "\n",
    "                pi     = actor(z_join)           # action scores\n",
    "                v      = critic(z_join)          # value\n",
    "                dist   = Categorical(logits=pi)  # to multinomial (discreet)\n",
    "                action = dist.sample()\n",
    "                log_pi = dist.log_prob(action).cpu().numpy()\n",
    "                action = action.cpu().numpy()\n",
    "                v = v.cpu().numpy()[:, 0]\n",
    "            \n",
    "            new_ob, reward, done, info = env.step(action)\n",
    "            self.store_transition(ob, log_pi, action, reward, v, done, info)\n",
    "            ob = self.preprocess_obs(new_ob)\n",
    "\n",
    "            if rollout_num % ROLLOUT_LENGTH == 0:\n",
    "                # bootstrap reward from critic predicted value function\n",
    "                # note that v is V(S_{t+1}) - TD(1) (one step returns)\n",
    "                with T.no_grad():\n",
    "                    tob = T.from_numpy(ob).to(DEVICE)\n",
    "                    z1 = ae_delta_1.encode(tob)      # latent repr of St+1\n",
    "                    z2 = ae_delta_2.encode(tob)      # latent repr of St+2\n",
    "                    z_join = T.cat((z1,z2), dim=1)   # joint latent repr\n",
    "                    flags = 1 - done   # should we bootstrap or not?\n",
    "                    v = critic(z_join)  # value of next state TD(1)\n",
    "                    v = v.cpu().numpy()[:, 0] * flags\n",
    "\n",
    "                # reward discounting / creit assignment\n",
    "                self.rewards[-1] += GAMMA * v\n",
    "                for i in reversed(range(ROLLOUT_LENGTH - 1)):\n",
    "                    flags = 1 - self.dones[i]\n",
    "                    self.rewards[i] += GAMMA * self.rewards[i+1] * flags\n",
    "\n",
    "                self.obs_d1 = self.obs[1:]         # for ae_delta_1\n",
    "                self.obs_d2 = self.obs[2:] + [ob]  # for ae_delta_2\n",
    "                del self.obs[-1]\n",
    "                del self.logps[-1]\n",
    "                del self.actions[-1]\n",
    "                del self.values[-1]\n",
    "                del self.rewards[-1]\n",
    "\n",
    "                # reshape and cast to torch tensors (rollouts)\n",
    "                self.flatten_rollout()\n",
    "                yield self.obs, self.obs_d1, self.obs_d2, self.logps, self.actions, self.values, self.rewards, self.infos\n",
    "                self.empty_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-FZ8_4gfJrt"
   },
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELVOSaxIfK5c"
   },
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "    def __init__(self,):\n",
    "        self.total_updates = int(TOTAL_TIMESTEPS // (NENV * ROLLOUT_LENGTH))\n",
    "        self.sma_ep_len    = deque(maxlen=50)\n",
    "        self.sma_ep_reward = deque(maxlen=50)\n",
    "        self.sma_pgloss    = deque(maxlen=250)\n",
    "        self.sma_vloss     = deque(maxlen=250)\n",
    "        self.sma_entropy   = deque(maxlen=250)\n",
    "        self.sma_loss      = deque(maxlen=250)\n",
    "        self.sma_ae1       = deque(maxlen=250)\n",
    "        self.sma_ae2       = deque(maxlen=250)\n",
    "        self.n_ep     = 0  # number of games/episodes\n",
    "        self.n_iters  = 0  # number of weight updates\n",
    "        self.n_frames = 0  # number of environment steps\n",
    "        self.max_reward = -np.inf\n",
    "        self.start_time = time.time()\n",
    "        # self.tb = TensorBoardColab()\n",
    "\n",
    "    def update(self, mb_info, pgloss, vloss, ent, loss, ae1_loss, ae2_loss, print_rate=250):\n",
    "        \"\"\" :param mb_info: dict containing episode lengths and rewards (if any) \n",
    "            :param print_rate: how often to print to stdout and flush TensorBoard\n",
    "            This saves the model to disk if we obtain a new high score.\n",
    "        \"\"\"\n",
    "        for i in range(len(mb_info['r'])):\n",
    "            ep_len = mb_info['l'][i]\n",
    "            ep_r   = mb_info['r'][i]\n",
    "            self.sma_ep_reward.append(ep_r)\n",
    "            self.sma_ep_len.append(ep_len)\n",
    "            # self.tb.save_value('Episode Length', 'ep_len', self.n_frames, ep_len)\n",
    "            # self.tb.save_value('Episode Reward', 'ep_r', self.n_frames, ep_r)\n",
    "            if ep_r > self.max_reward:\n",
    "                if self.n_frames > 500000:\n",
    "                    ppo_save_path = SAVE_PATH.format(ENV_NAME+'-PPO', ep_r)\n",
    "                    rnd_save_path = SAVE_PATH.format(ENV_NAME+'-RND', ep_r)\n",
    "                    T.save(ppo_network.state_dict(), ppo_save_path)  # save model to disk\n",
    "                    T.save(rnd_network.state_dict(), rnd_save_path)  # save model to disk\n",
    "                self.max_reward = ep_r\n",
    "            self.n_ep += 1\n",
    "\n",
    "\n",
    "        self.n_iters += 1\n",
    "        self.n_frames += (ROLLOUT_LENGTH*NENV)\n",
    "\n",
    "        self.sma_pgloss.append(pgloss)\n",
    "        self.sma_vloss.append(vloss)\n",
    "        self.sma_entropy.append(ent)\n",
    "        self.sma_loss.append(loss)\n",
    "        self.sma_ae1.append(ae1_loss)\n",
    "        self.sma_ae2.append(ae2_loss)\n",
    "\n",
    "        # log raw values to TensorBoard\n",
    "        # self.tb.save_value('Loss', 'loss', self.n_frames, loss)\n",
    "        # self.tb.save_value('Entropy', 'ent', self.n_frames, ent)\n",
    "        # self.tb.save_value('Value Loss', 'vloss', self.n_frames, vloss)\n",
    "        # self.tb.save_value('PG Loss', 'pgloss', self.n_frames, pgloss)\n",
    "        # self.tb.save_value('Intrinsic Loss', 'intrinsic', self.n_frames, intr)\n",
    "\n",
    "        if self.n_iters % print_rate == 0:\n",
    "            # self.tb.flush_line('ep_len')\n",
    "            # self.tb.flush_line('ep_r')\n",
    "            # self.tb.flush_line('ent')\n",
    "            # self.tb.flush_line('vloss')\n",
    "            # self.tb.flush_line('pgloss')\n",
    "            # self.tb.flush_line('loss')\n",
    "            # self.tb.flush_line('intrinsic')\n",
    "            self.report()\n",
    "\n",
    "    def report(self):\n",
    "        \"\"\" print stats to stdout \"\"\"\n",
    "        if len(self.sma_ep_len) <= 0:\n",
    "            return\n",
    "        elapsed_time = time.time()-self.start_time\n",
    "        fps = int(self.n_frames / elapsed_time)\n",
    "        eta = (TOTAL_TIMESTEPS - self.n_frames) * (elapsed_time / self.n_frames)\n",
    "        eta_str = str(datetime.timedelta(seconds=int(eta)))\n",
    "        print('-'*10, self.n_iters, '/', self.total_updates, '-'*10)\n",
    "        print(f'SMA 50 Length: {sum(self.sma_ep_len)/len(self.sma_ep_len)}')\n",
    "        print(f'SMA 50 Reward: {sum(self.sma_ep_reward)/len(self.sma_ep_reward)}')\n",
    "        print(f'SMA 250 pgloss: {sum(self.sma_pgloss)/len(self.sma_pgloss)}')\n",
    "        print(f'SMA 250 vloss: {sum(self.sma_vloss)/len(self.sma_vloss)}')\n",
    "        print(f'SMA 250 ae1 loss: {sum(self.sma_ae1)/len(self.sma_ae1)}')\n",
    "        print(f'SMA 250 ae2 loss: {sum(self.sma_ae2)/len(self.sma_ae2)}')\n",
    "        print(f'SMA 250 loss: {sum(self.sma_loss)/len(self.sma_loss)}')\n",
    "        print(f'SMA 250 entropy: {sum(self.sma_entropy)/len(self.sma_entropy)}')\n",
    "        print(f'FPS: {fps}')\n",
    "        print(f'ETA: {eta_str}')\n",
    "        print(f'Max reward: {self.max_reward}')\n",
    "        print(f'Number of games: {self.n_ep}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Vqc0ebOkJFZ"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 652,
     "status": "ok",
     "timestamp": 1582313331463,
     "user": {
      "displayName": "harry songhurst",
      "photoUrl": "",
      "userId": "00629498674930426914"
     },
     "user_tz": 0
    },
    "id": "Wafv_YXj4ZP_",
    "outputId": "f13044c5-588f-4aea-f034-15a1ee3f20c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type AutoEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# this autoencoder predicts S_{t+1}.\n",
    "ae_delta_1 = AutoEncoder(in_dim, num_actions, LATENT_SIZE).to(DEVICE)\n",
    "ae_optim_1 = Adam(ae_delta_1.parameters(), 7e-4, eps=1e-5)\n",
    "T.save(ae_delta_1, 'clone.t')\n",
    "\n",
    "# this autoencoder predicts S_{t+2}. \n",
    "# cloned so latent dynamics are similar.\n",
    "ae_delta_2 = T.load('clone.t').to(DEVICE)\n",
    "ae_optim_2 = Adam(ae_delta_2.parameters(), 7e-4, eps=1e-5)\n",
    "\n",
    "# this is our actor + critic\n",
    "# input is both latent vectors of above autoencoders.\n",
    "actor = ACTOR(LATENT_SIZE*2, num_actions).to(DEVICE)\n",
    "actor_optim = Adam(actor.parameters(), 7e-4, eps=1e-5)\n",
    "\n",
    "critic = CRITIC(LATENT_SIZE*2, num_actions).to(DEVICE)\n",
    "critic_optim = Adam(critic.parameters(), 7e-4, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CBQu-7iekISu"
   },
   "outputs": [],
   "source": [
    "MB_LEN      = int(NENV*(ROLLOUT_LENGTH-1))\n",
    "SPLIT_LEN   = int(MB_LEN//MB_SPLIT)\n",
    "NUM_UPDATES = int(TOTAL_TIMESTEPS//MB_LEN)\n",
    "\n",
    "rgen = Rollouts().generator()\n",
    "\n",
    "actor_scheduler  = LambdaLR(actor_optim, lambda i: 1 - (i / NUM_UPDATES))\n",
    "critic_scheduler  = LambdaLR(critic_optim, lambda i: 1 - (i / NUM_UPDATES))\n",
    "ae_1_scheduler = LambdaLR(ae_optim_1, lambda i: 1 - (i / NUM_UPDATES))\n",
    "ae_2_scheduler = LambdaLR(ae_optim_2, lambda i: 1 - (i / NUM_UPDATES))\n",
    "\n",
    "logger = Logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 933525,
     "status": "error",
     "timestamp": 1582314266397,
     "user": {
      "displayName": "harry songhurst",
      "photoUrl": "",
      "userId": "00629498674930426914"
     },
     "user_tz": 0
    },
    "id": "rWiooI_nyZ0n",
    "outputId": "a9d556b2-6e63-43a7-d9e1-653151cde668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing 49 repr updates...\n",
      "Doing 48 repr updates...\n",
      "Doing 47 repr updates...\n",
      "Doing 46 repr updates...\n",
      "Doing 45 repr updates...\n",
      "---------- 5 / 4882 ----------\n",
      "SMA 50 Length: 185.22857142857143\n",
      "SMA 50 Reward: 2.2\n",
      "SMA 250 pgloss: 0.019152600690722464\n",
      "SMA 250 vloss: 0.12003977596759796\n",
      "SMA 250 ae1 loss: 0.007770133204758167\n",
      "SMA 250 ae2 loss: 0.008472253940999508\n",
      "SMA 250 loss: 0.009988954663276673\n",
      "SMA 250 entropy: 0.9163646459579468\n",
      "FPS: 96\n",
      "ETA: 14:23:18\n",
      "Max reward: 11.0\n",
      "Number of games: 35\n",
      "SOFTMAX:  tensor([0.0158, 0.0878, 0.0100, 0.8865], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 44 repr updates...\n",
      "Doing 43 repr updates...\n",
      "Doing 42 repr updates...\n",
      "Doing 41 repr updates...\n",
      "Doing 40 repr updates...\n",
      "---------- 10 / 4882 ----------\n",
      "SMA 50 Length: 167.72\n",
      "SMA 50 Reward: 2.38\n",
      "SMA 250 pgloss: 0.009352510992903263\n",
      "SMA 250 vloss: 0.14534263610839843\n",
      "SMA 250 ae1 loss: 0.005544279422610998\n",
      "SMA 250 ae2 loss: 0.006202958128415048\n",
      "SMA 250 loss: 0.0011211581993848085\n",
      "SMA 250 entropy: 0.8231353163719177\n",
      "FPS: 101\n",
      "ETA: 13:36:31\n",
      "Max reward: 11.0\n",
      "Number of games: 66\n",
      "SOFTMAX:  tensor([0.0550, 0.5123, 0.0236, 0.4091], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 39 repr updates...\n",
      "Doing 38 repr updates...\n",
      "Doing 37 repr updates...\n",
      "Doing 36 repr updates...\n",
      "Doing 35 repr updates...\n",
      "---------- 15 / 4882 ----------\n",
      "SMA 50 Length: 181.12\n",
      "SMA 50 Reward: 2.4\n",
      "SMA 250 pgloss: 0.009216655239773294\n",
      "SMA 250 vloss: 0.16459966997305553\n",
      "SMA 250 ae1 loss: 0.004726408328860998\n",
      "SMA 250 ae2 loss: 0.005374135651315252\n",
      "SMA 250 loss: 0.000590111097941796\n",
      "SMA 250 entropy: 0.862654447555542\n",
      "FPS: 106\n",
      "ETA: 12:59:09\n",
      "Max reward: 11.0\n",
      "Number of games: 94\n",
      "SOFTMAX:  tensor([0.0615, 0.6256, 0.1902, 0.1226], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 34 repr updates...\n",
      "Doing 33 repr updates...\n",
      "Doing 32 repr updates...\n",
      "Doing 31 repr updates...\n",
      "Doing 30 repr updates...\n",
      "---------- 20 / 4882 ----------\n",
      "SMA 50 Length: 194.98\n",
      "SMA 50 Reward: 1.98\n",
      "SMA 250 pgloss: 0.006946721608983353\n",
      "SMA 250 vloss: 0.17516062036156654\n",
      "SMA 250 ae1 loss: 0.004180704418104142\n",
      "SMA 250 ae2 loss: 0.004853696213103831\n",
      "SMA 250 loss: -0.002729044551961124\n",
      "SMA 250 entropy: 0.9675766468048096\n",
      "FPS: 111\n",
      "ETA: 12:22:22\n",
      "Max reward: 11.0\n",
      "Number of games: 123\n",
      "SOFTMAX:  tensor([0.4218, 0.2252, 0.2526, 0.1005], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 29 repr updates...\n",
      "Doing 28 repr updates...\n",
      "Doing 27 repr updates...\n",
      "Doing 26 repr updates...\n",
      "Doing 25 repr updates...\n",
      "---------- 25 / 4882 ----------\n",
      "SMA 50 Length: 204.96\n",
      "SMA 50 Reward: 2.02\n",
      "SMA 250 pgloss: 0.008225087700411677\n",
      "SMA 250 vloss: 0.18495477974414826\n",
      "SMA 250 ae1 loss: 0.003837606683373451\n",
      "SMA 250 ae2 loss: 0.00456363720819354\n",
      "SMA 250 loss: -0.002030434999614954\n",
      "SMA 250 entropy: 1.0255522966384887\n",
      "FPS: 117\n",
      "ETA: 11:46:38\n",
      "Max reward: 11.0\n",
      "Number of games: 148\n",
      "SOFTMAX:  tensor([0.2983, 0.4273, 0.2037, 0.0707], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 24 repr updates...\n",
      "Doing 23 repr updates...\n",
      "Doing 22 repr updates...\n",
      "Doing 21 repr updates...\n",
      "Doing 20 repr updates...\n",
      "---------- 30 / 4882 ----------\n",
      "SMA 50 Length: 216.18\n",
      "SMA 50 Reward: 2.34\n",
      "SMA 250 pgloss: 0.012810600354957085\n",
      "SMA 250 vloss: 0.19398602743943533\n",
      "SMA 250 ae1 loss: 0.0035559286596253516\n",
      "SMA 250 ae2 loss: 0.004300479311496019\n",
      "SMA 250 loss: 0.0022412890413155157\n",
      "SMA 250 entropy: 1.0569311698277792\n",
      "FPS: 123\n",
      "ETA: 11:10:06\n",
      "Max reward: 11.0\n",
      "Number of games: 172\n",
      "SOFTMAX:  tensor([0.4027, 0.2707, 0.2888, 0.0378], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 19 repr updates...\n",
      "Doing 18 repr updates...\n",
      "Doing 17 repr updates...\n",
      "Doing 16 repr updates...\n",
      "Doing 15 repr updates...\n",
      "---------- 35 / 4882 ----------\n",
      "SMA 50 Length: 217.7\n",
      "SMA 50 Reward: 2.4\n",
      "SMA 250 pgloss: 0.00951261584913092\n",
      "SMA 250 vloss: 0.19590761555092676\n",
      "SMA 250 ae1 loss: 0.0033937496532286918\n",
      "SMA 250 ae2 loss: 0.004176213672118527\n",
      "SMA 250 loss: -0.0013287558353372983\n",
      "SMA 250 entropy: 1.084137214933123\n",
      "FPS: 130\n",
      "ETA: 10:33:59\n",
      "Max reward: 11.0\n",
      "Number of games: 200\n",
      "SOFTMAX:  tensor([0.3114, 0.2563, 0.3464, 0.0859], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 14 repr updates...\n",
      "Doing 13 repr updates...\n",
      "Doing 12 repr updates...\n",
      "Doing 11 repr updates...\n",
      "Doing 10 repr updates...\n",
      "---------- 40 / 4882 ----------\n",
      "SMA 50 Length: 180.92\n",
      "SMA 50 Reward: 1.38\n",
      "SMA 250 pgloss: 0.015386463218601421\n",
      "SMA 250 vloss: 0.19701507389545442\n",
      "SMA 250 ae1 loss: 0.0032971520209684967\n",
      "SMA 250 ae2 loss: 0.004121237777872011\n",
      "SMA 250 loss: 0.004252538387663663\n",
      "SMA 250 entropy: 1.1133925199508667\n",
      "FPS: 138\n",
      "ETA: 9:57:30\n",
      "Max reward: 11.0\n",
      "Number of games: 233\n",
      "SOFTMAX:  tensor([0.2965, 0.2773, 0.2945, 0.1316], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 9 repr updates...\n",
      "Doing 8 repr updates...\n",
      "Doing 7 repr updates...\n",
      "Doing 6 repr updates...\n",
      "Doing 5 repr updates...\n",
      "---------- 45 / 4882 ----------\n",
      "SMA 50 Length: 196.18\n",
      "SMA 50 Reward: 1.82\n",
      "SMA 250 pgloss: 0.010954497532091207\n",
      "SMA 250 vloss: 0.2028996048702134\n",
      "SMA 250 ae1 loss: 0.0031972411347346175\n",
      "SMA 250 ae2 loss: 0.004035554407164454\n",
      "SMA 250 loss: -0.00046587029678954017\n",
      "SMA 250 entropy: 1.1420368247561985\n",
      "FPS: 147\n",
      "ETA: 9:21:33\n",
      "Max reward: 12.0\n",
      "Number of games: 258\n",
      "SOFTMAX:  tensor([0.2757, 0.2042, 0.2873, 0.2328], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 4 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 50 / 4882 ----------\n",
      "SMA 50 Length: 192.08\n",
      "SMA 50 Reward: 1.58\n",
      "SMA 250 pgloss: 0.017044954360462727\n",
      "SMA 250 vloss: 0.2000921155512333\n",
      "SMA 250 ae1 loss: 0.003155981618911028\n",
      "SMA 250 ae2 loss: 0.0040232182992622255\n",
      "SMA 250 loss: 0.0054013200849294665\n",
      "SMA 250 entropy: 1.1643634605407716\n",
      "FPS: 156\n",
      "ETA: 8:47:12\n",
      "Max reward: 12.0\n",
      "Number of games: 292\n",
      "SOFTMAX:  tensor([0.3056, 0.1667, 0.3146, 0.2131], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 55 / 4882 ----------\n",
      "SMA 50 Length: 183.22\n",
      "SMA 50 Reward: 1.38\n",
      "SMA 250 pgloss: 0.013118658913299441\n",
      "SMA 250 vloss: 0.19849172776395624\n",
      "SMA 250 ae1 loss: 0.0030972055752169\n",
      "SMA 250 ae2 loss: 0.003979310334067453\n",
      "SMA 250 loss: 0.001315659758719531\n",
      "SMA 250 entropy: 1.1802999626506459\n",
      "FPS: 165\n",
      "ETA: 8:18:33\n",
      "Max reward: 12.0\n",
      "Number of games: 319\n",
      "SOFTMAX:  tensor([0.3532, 0.1441, 0.3000, 0.2027], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 60 / 4882 ----------\n",
      "SMA 50 Length: 194.42\n",
      "SMA 50 Reward: 1.66\n",
      "SMA 250 pgloss: 0.016946547218443204\n",
      "SMA 250 vloss: 0.20189662923415502\n",
      "SMA 250 ae1 loss: 0.0030491826590150596\n",
      "SMA 250 ae2 loss: 0.00394521802275752\n",
      "SMA 250 loss: 0.005014311739554008\n",
      "SMA 250 entropy: 1.1932235995928446\n",
      "FPS: 173\n",
      "ETA: 7:54:30\n",
      "Max reward: 12.0\n",
      "Number of games: 349\n",
      "SOFTMAX:  tensor([0.3397, 0.1377, 0.3004, 0.2222], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 65 / 4882 ----------\n",
      "SMA 50 Length: 186.92\n",
      "SMA 50 Reward: 1.46\n",
      "SMA 250 pgloss: 0.011776845604897692\n",
      "SMA 250 vloss: 0.20222525642468378\n",
      "SMA 250 ae1 loss: 0.0030144610191480473\n",
      "SMA 250 ae2 loss: 0.003920656274287747\n",
      "SMA 250 loss: -0.0002635097560974268\n",
      "SMA 250 entropy: 1.2040355939131517\n",
      "FPS: 180\n",
      "ETA: 7:34:20\n",
      "Max reward: 12.0\n",
      "Number of games: 379\n",
      "SOFTMAX:  tensor([0.3017, 0.1216, 0.3489, 0.2278], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 70 / 4882 ----------\n",
      "SMA 50 Length: 209.06\n",
      "SMA 50 Reward: 2.1\n",
      "SMA 250 pgloss: 0.015768865086803478\n",
      "SMA 250 vloss: 0.2040173283645085\n",
      "SMA 250 ae1 loss: 0.0029654734335573655\n",
      "SMA 250 ae2 loss: 0.003873428249997752\n",
      "SMA 250 loss: 0.003655106441250869\n",
      "SMA 250 entropy: 1.2113759160041808\n",
      "FPS: 187\n",
      "ETA: 7:17:08\n",
      "Max reward: 13.0\n",
      "Number of games: 403\n",
      "SOFTMAX:  tensor([0.3188, 0.1140, 0.3889, 0.1783], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 75 / 4882 ----------\n",
      "SMA 50 Length: 195.8\n",
      "SMA 50 Reward: 1.88\n",
      "SMA 250 pgloss: 0.01202217692664514\n",
      "SMA 250 vloss: 0.20287913938363394\n",
      "SMA 250 ae1 loss: 0.002950123402600487\n",
      "SMA 250 ae2 loss: 0.0038709219203641016\n",
      "SMA 250 loss: -0.00012108941872914632\n",
      "SMA 250 entropy: 1.2143267027537028\n",
      "FPS: 194\n",
      "ETA: 7:02:04\n",
      "Max reward: 13.0\n",
      "Number of games: 435\n",
      "SOFTMAX:  tensor([0.3364, 0.1196, 0.4057, 0.1382], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 80 / 4882 ----------\n",
      "SMA 50 Length: 189.16\n",
      "SMA 50 Reward: 1.74\n",
      "SMA 250 pgloss: 0.007861383254930843\n",
      "SMA 250 vloss: 0.20120223108679056\n",
      "SMA 250 ae1 loss: 0.0029228898842120544\n",
      "SMA 250 ae2 loss: 0.0038504714786540715\n",
      "SMA 250 loss: -0.004321875283494592\n",
      "SMA 250 entropy: 1.2183259278535843\n",
      "FPS: 200\n",
      "ETA: 6:48:45\n",
      "Max reward: 13.0\n",
      "Number of games: 465\n",
      "SOFTMAX:  tensor([0.3738, 0.1435, 0.3311, 0.1516], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 85 / 4882 ----------\n",
      "SMA 50 Length: 203.3\n",
      "SMA 50 Reward: 1.98\n",
      "SMA 250 pgloss: 0.009980426549308878\n",
      "SMA 250 vloss: 0.20289486646652222\n",
      "SMA 250 ae1 loss: 0.0028972952051416916\n",
      "SMA 250 ae2 loss: 0.0038299604045117603\n",
      "SMA 250 loss: -0.002241621122640722\n",
      "SMA 250 entropy: 1.2222048408844892\n",
      "FPS: 206\n",
      "ETA: 6:36:56\n",
      "Max reward: 13.0\n",
      "Number of games: 493\n",
      "SOFTMAX:  tensor([0.4649, 0.1116, 0.2893, 0.1343], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 90 / 4882 ----------\n",
      "SMA 50 Length: 195.34\n",
      "SMA 50 Reward: 1.66\n",
      "SMA 250 pgloss: 0.009190284663863067\n",
      "SMA 250 vloss: 0.2017841898732715\n",
      "SMA 250 ae1 loss: 0.002872564152090086\n",
      "SMA 250 ae2 loss: 0.0038092911786710222\n",
      "SMA 250 loss: -0.0030111363985472254\n",
      "SMA 250 entropy: 1.2201421817143758\n",
      "FPS: 211\n",
      "ETA: 6:26:55\n",
      "Max reward: 13.0\n",
      "Number of games: 521\n",
      "SOFTMAX:  tensor([0.5050, 0.0777, 0.2875, 0.1298], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 95 / 4882 ----------\n",
      "SMA 50 Length: 196.64\n",
      "SMA 50 Reward: 1.82\n",
      "SMA 250 pgloss: 0.009550709410962697\n",
      "SMA 250 vloss: 0.20142512729293421\n",
      "SMA 250 ae1 loss: 0.0028473141186527518\n",
      "SMA 250 ae2 loss: 0.003784783289914853\n",
      "SMA 250 loss: -0.0026353572348230764\n",
      "SMA 250 entropy: 1.2186067280016448\n",
      "FPS: 216\n",
      "ETA: 6:17:23\n",
      "Max reward: 13.0\n",
      "Number of games: 548\n",
      "SOFTMAX:  tensor([0.4658, 0.0641, 0.3133, 0.1568], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 100 / 4882 ----------\n",
      "SMA 50 Length: 186.94\n",
      "SMA 50 Reward: 1.68\n",
      "SMA 250 pgloss: 0.008161158725852147\n",
      "SMA 250 vloss: 0.20344373799860477\n",
      "SMA 250 ae1 loss: 0.002830939486157149\n",
      "SMA 250 ae2 loss: 0.0037735547847114505\n",
      "SMA 250 loss: -0.004009563531726598\n",
      "SMA 250 entropy: 1.2170722830295562\n",
      "FPS: 221\n",
      "ETA: 6:08:51\n",
      "Max reward: 13.0\n",
      "Number of games: 578\n",
      "SOFTMAX:  tensor([0.4282, 0.0591, 0.3409, 0.1718], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 105 / 4882 ----------\n",
      "SMA 50 Length: 191.48\n",
      "SMA 50 Reward: 1.92\n",
      "SMA 250 pgloss: 0.007764030830003321\n",
      "SMA 250 vloss: 0.20292486881925947\n",
      "SMA 250 ae1 loss: 0.0028085878185395686\n",
      "SMA 250 ae2 loss: 0.003750543123377221\n",
      "SMA 250 loss: -0.004385608292761303\n",
      "SMA 250 entropy: 1.2149639617829096\n",
      "FPS: 225\n",
      "ETA: 6:01:01\n",
      "Max reward: 13.0\n",
      "Number of games: 604\n",
      "SOFTMAX:  tensor([0.3407, 0.0375, 0.4755, 0.1463], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 110 / 4882 ----------\n",
      "SMA 50 Length: 217.2\n",
      "SMA 50 Reward: 2.76\n",
      "SMA 250 pgloss: 0.008589787659531629\n",
      "SMA 250 vloss: 0.2040585574101318\n",
      "SMA 250 ae1 loss: 0.0027834113052284176\n",
      "SMA 250 ae2 loss: 0.003724868811497634\n",
      "SMA 250 loss: -0.003503041782162406\n",
      "SMA 250 entropy: 1.2092829921028831\n",
      "FPS: 229\n",
      "ETA: 5:54:17\n",
      "Max reward: 13.0\n",
      "Number of games: 630\n",
      "SOFTMAX:  tensor([0.2767, 0.1112, 0.4041, 0.2080], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 115 / 4882 ----------\n",
      "SMA 50 Length: 199.38\n",
      "SMA 50 Reward: 2.1\n",
      "SMA 250 pgloss: 0.007165439602028092\n",
      "SMA 250 vloss: 0.2045027243702308\n",
      "SMA 250 ae1 loss: 0.0027679349437517964\n",
      "SMA 250 ae2 loss: 0.0037103592073949782\n",
      "SMA 250 loss: -0.004892540463934774\n",
      "SMA 250 entropy: 1.20579805166825\n",
      "FPS: 233\n",
      "ETA: 5:48:03\n",
      "Max reward: 13.0\n",
      "Number of games: 657\n",
      "SOFTMAX:  tensor([0.2488, 0.0572, 0.5027, 0.1913], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 120 / 4882 ----------\n",
      "SMA 50 Length: 218.48\n",
      "SMA 50 Reward: 2.6\n",
      "SMA 250 pgloss: 0.00963008526887279\n",
      "SMA 250 vloss: 0.2064644709850351\n",
      "SMA 250 ae1 loss: 0.0027451879936658467\n",
      "SMA 250 ae2 loss: 0.00368287864063556\n",
      "SMA 250 loss: -0.002444774207348625\n",
      "SMA 250 entropy: 1.2074859817822774\n",
      "FPS: 237\n",
      "ETA: 5:41:57\n",
      "Max reward: 15.0\n",
      "Number of games: 683\n",
      "SOFTMAX:  tensor([0.2651, 0.1710, 0.3266, 0.2374], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 125 / 4882 ----------\n",
      "SMA 50 Length: 199.6\n",
      "SMA 50 Reward: 2.04\n",
      "SMA 250 pgloss: 0.009056888711638749\n",
      "SMA 250 vloss: 0.20343397557735443\n",
      "SMA 250 ae1 loss: 0.002749039185233414\n",
      "SMA 250 ae2 loss: 0.0036945991013199093\n",
      "SMA 250 loss: -0.003076757475733757\n",
      "SMA 250 entropy: 1.21336465549469\n",
      "FPS: 241\n",
      "ETA: 5:36:31\n",
      "Max reward: 15.0\n",
      "Number of games: 716\n",
      "SOFTMAX:  tensor([0.2731, 0.1881, 0.2992, 0.2397], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 130 / 4882 ----------\n",
      "SMA 50 Length: 164.72\n",
      "SMA 50 Reward: 1.02\n",
      "SMA 250 pgloss: 0.009188343639163156\n",
      "SMA 250 vloss: 0.20196902207457101\n",
      "SMA 250 ae1 loss: 0.0027547373964737815\n",
      "SMA 250 ae2 loss: 0.003708394681318448\n",
      "SMA 250 loss: -0.0030104102853399056\n",
      "SMA 250 entropy: 1.2198754255588238\n",
      "FPS: 244\n",
      "ETA: 5:31:39\n",
      "Max reward: 15.0\n",
      "Number of games: 750\n",
      "SOFTMAX:  tensor([0.2533, 0.2279, 0.2647, 0.2541], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 135 / 4882 ----------\n",
      "SMA 50 Length: 165.58\n",
      "SMA 50 Reward: 0.94\n",
      "SMA 250 pgloss: 0.007439747298377808\n",
      "SMA 250 vloss: 0.2005218109598866\n",
      "SMA 250 ae1 loss: 0.002758044726215303\n",
      "SMA 250 ae2 loss: 0.0037195353837752784\n",
      "SMA 250 loss: -0.004820286227321184\n",
      "SMA 250 entropy: 1.2260033863562125\n",
      "FPS: 247\n",
      "ETA: 5:27:13\n",
      "Max reward: 15.0\n",
      "Number of games: 784\n",
      "SOFTMAX:  tensor([0.2416, 0.2398, 0.2565, 0.2621], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 140 / 4882 ----------\n",
      "SMA 50 Length: 160.74\n",
      "SMA 50 Reward: 0.86\n",
      "SMA 250 pgloss: 0.008161373779876158\n",
      "SMA 250 vloss: 0.197939575782844\n",
      "SMA 250 ae1 loss: 0.002760873903753236\n",
      "SMA 250 ae2 loss: 0.0037273155758157372\n",
      "SMA 250 loss: -0.004155576747975179\n",
      "SMA 250 entropy: 1.2316950866154261\n",
      "FPS: 250\n",
      "ETA: 5:22:55\n",
      "Max reward: 15.0\n",
      "Number of games: 818\n",
      "SOFTMAX:  tensor([0.2484, 0.2401, 0.2474, 0.2641], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 145 / 4882 ----------\n",
      "SMA 50 Length: 180.04\n",
      "SMA 50 Reward: 1.32\n",
      "SMA 250 pgloss: 0.007652966662888126\n",
      "SMA 250 vloss: 0.19947224886252962\n",
      "SMA 250 ae1 loss: 0.002754364441277395\n",
      "SMA 250 ae2 loss: 0.003722371533513069\n",
      "SMA 250 loss: -0.0047171960488475604\n",
      "SMA 250 entropy: 1.2370162963867188\n",
      "FPS: 253\n",
      "ETA: 5:18:45\n",
      "Max reward: 15.0\n",
      "Number of games: 847\n",
      "SOFTMAX:  tensor([0.2474, 0.2440, 0.2597, 0.2489], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 150 / 4882 ----------\n",
      "SMA 50 Length: 187.44\n",
      "SMA 50 Reward: 1.56\n",
      "SMA 250 pgloss: 0.008262289722915738\n",
      "SMA 250 vloss: 0.1998035286863645\n",
      "SMA 250 ae1 loss: 0.002746332797687501\n",
      "SMA 250 ae2 loss: 0.003713512623993059\n",
      "SMA 250 loss: -0.0041573842739065486\n",
      "SMA 250 entropy: 1.2419674245516459\n",
      "FPS: 256\n",
      "ETA: 5:14:50\n",
      "Max reward: 15.0\n",
      "Number of games: 875\n",
      "SOFTMAX:  tensor([0.2463, 0.2415, 0.2697, 0.2426], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 155 / 4882 ----------\n",
      "SMA 50 Length: 200.96\n",
      "SMA 50 Reward: 1.84\n",
      "SMA 250 pgloss: 0.0075143792313493545\n",
      "SMA 250 vloss: 0.20042093184686477\n",
      "SMA 250 ae1 loss: 0.0027400785313558673\n",
      "SMA 250 ae2 loss: 0.0037062218322628927\n",
      "SMA 250 loss: -0.004950999552684445\n",
      "SMA 250 entropy: 1.2465379038164692\n",
      "FPS: 259\n",
      "ETA: 5:11:31\n",
      "Max reward: 15.0\n",
      "Number of games: 904\n",
      "SOFTMAX:  tensor([0.2407, 0.2231, 0.2915, 0.2447], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 160 / 4882 ----------\n",
      "SMA 50 Length: 200.02\n",
      "SMA 50 Reward: 1.76\n",
      "SMA 250 pgloss: 0.008702383207491948\n",
      "SMA 250 vloss: 0.20189342889934778\n",
      "SMA 250 ae1 loss: 0.0027249833197856787\n",
      "SMA 250 ae2 loss: 0.0036869473624392413\n",
      "SMA 250 loss: -0.0037991284276358782\n",
      "SMA 250 entropy: 1.2501511864364148\n",
      "FPS: 261\n",
      "ETA: 5:08:00\n",
      "Max reward: 15.0\n",
      "Number of games: 926\n",
      "SOFTMAX:  tensor([0.2184, 0.1859, 0.3477, 0.2480], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 165 / 4882 ----------\n",
      "SMA 50 Length: 200.28\n",
      "SMA 50 Reward: 1.82\n",
      "SMA 250 pgloss: 0.007528025515476298\n",
      "SMA 250 vloss: 0.20308543994571224\n",
      "SMA 250 ae1 loss: 0.0027173560528987737\n",
      "SMA 250 ae2 loss: 0.003676261781065753\n",
      "SMA 250 loss: -0.005008298147356871\n",
      "SMA 250 entropy: 1.2536323944727579\n",
      "FPS: 264\n",
      "ETA: 5:04:45\n",
      "Max reward: 15.0\n",
      "Number of games: 954\n",
      "SOFTMAX:  tensor([0.2457, 0.2104, 0.3001, 0.2438], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 170 / 4882 ----------\n",
      "SMA 50 Length: 192.62\n",
      "SMA 50 Reward: 1.64\n",
      "SMA 250 pgloss: 0.006621957153616035\n",
      "SMA 250 vloss: 0.20293283230241607\n",
      "SMA 250 ae1 loss: 0.002717002381926731\n",
      "SMA 250 ae2 loss: 0.0036763356628772966\n",
      "SMA 250 loss: -0.005952312835656544\n",
      "SMA 250 entropy: 1.2574270262437708\n",
      "FPS: 266\n",
      "ETA: 5:01:46\n",
      "Max reward: 15.0\n",
      "Number of games: 986\n",
      "SOFTMAX:  tensor([0.2540, 0.2286, 0.2770, 0.2404], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 175 / 4882 ----------\n",
      "SMA 50 Length: 191.46\n",
      "SMA 50 Reward: 1.68\n",
      "SMA 250 pgloss: 0.009813674688006618\n",
      "SMA 250 vloss: 0.2054723668524197\n",
      "SMA 250 ae1 loss: 0.0027087598486936516\n",
      "SMA 250 ae2 loss: 0.003665018659085035\n",
      "SMA 250 loss: -0.002796717451087066\n",
      "SMA 250 entropy: 1.2610392461504254\n",
      "FPS: 268\n",
      "ETA: 4:58:54\n",
      "Max reward: 15.0\n",
      "Number of games: 1011\n",
      "SOFTMAX:  tensor([0.2579, 0.2285, 0.2747, 0.2388], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 180 / 4882 ----------\n",
      "SMA 50 Length: 200.0\n",
      "SMA 50 Reward: 1.94\n",
      "SMA 250 pgloss: 0.008970163193104478\n",
      "SMA 250 vloss: 0.2058835352046622\n",
      "SMA 250 ae1 loss: 0.0027036309248716054\n",
      "SMA 250 ae2 loss: 0.003659180355154806\n",
      "SMA 250 loss: -0.003673946148612433\n",
      "SMA 250 entropy: 1.264410964647929\n",
      "FPS: 270\n",
      "ETA: 4:56:16\n",
      "Max reward: 15.0\n",
      "Number of games: 1040\n",
      "SOFTMAX:  tensor([0.2646, 0.2197, 0.2840, 0.2317], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 185 / 4882 ----------\n",
      "SMA 50 Length: 194.72\n",
      "SMA 50 Reward: 1.66\n",
      "SMA 250 pgloss: 0.009106370937267972\n",
      "SMA 250 vloss: 0.20699752363804225\n",
      "SMA 250 ae1 loss: 0.0026962653928864246\n",
      "SMA 250 ae2 loss: 0.0036510825736095777\n",
      "SMA 250 loss: -0.0035690026861187572\n",
      "SMA 250 entropy: 1.2675374082616857\n",
      "FPS: 273\n",
      "ETA: 4:53:38\n",
      "Max reward: 15.0\n",
      "Number of games: 1068\n",
      "SOFTMAX:  tensor([0.2685, 0.2182, 0.2873, 0.2260], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 190 / 4882 ----------\n",
      "SMA 50 Length: 193.48\n",
      "SMA 50 Reward: 1.66\n",
      "SMA 250 pgloss: 0.008039519494387174\n",
      "SMA 250 vloss: 0.20699524397128508\n",
      "SMA 250 ae1 loss: 0.002692836629055244\n",
      "SMA 250 ae2 loss: 0.0036468210764915537\n",
      "SMA 250 loss: -0.004665043513829771\n",
      "SMA 250 entropy: 1.2704563454577797\n",
      "FPS: 275\n",
      "ETA: 4:51:09\n",
      "Max reward: 15.0\n",
      "Number of games: 1098\n",
      "SOFTMAX:  tensor([0.2827, 0.2123, 0.2841, 0.2209], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 195 / 4882 ----------\n",
      "SMA 50 Length: 186.62\n",
      "SMA 50 Reward: 1.56\n",
      "SMA 250 pgloss: 0.008106426125428138\n",
      "SMA 250 vloss: 0.20696722249954175\n",
      "SMA 250 ae1 loss: 0.0026876070781443745\n",
      "SMA 250 ae2 loss: 0.0036411575973033907\n",
      "SMA 250 loss: -0.004625167883932591\n",
      "SMA 250 entropy: 1.2731594452491173\n",
      "FPS: 277\n",
      "ETA: 4:48:45\n",
      "Max reward: 15.0\n",
      "Number of games: 1125\n",
      "SOFTMAX:  tensor([0.3033, 0.2215, 0.2671, 0.2080], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 200 / 4882 ----------\n",
      "SMA 50 Length: 193.26\n",
      "SMA 50 Reward: 1.68\n",
      "SMA 250 pgloss: 0.0076383050947333685\n",
      "SMA 250 vloss: 0.20691298808902503\n",
      "SMA 250 ae1 loss: 0.0026854006020585073\n",
      "SMA 250 ae2 loss: 0.0036395920591894538\n",
      "SMA 250 loss: -0.005117239179089666\n",
      "SMA 250 entropy: 1.275554473400116\n",
      "FPS: 278\n",
      "ETA: 4:46:40\n",
      "Max reward: 15.0\n",
      "Number of games: 1155\n",
      "SOFTMAX:  tensor([0.3547, 0.2162, 0.2491, 0.1800], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 205 / 4882 ----------\n",
      "SMA 50 Length: 195.42\n",
      "SMA 50 Reward: 1.68\n",
      "SMA 250 pgloss: 0.01086043777688202\n",
      "SMA 250 vloss: 0.20808686668553003\n",
      "SMA 250 ae1 loss: 0.002671271100322284\n",
      "SMA 250 ae2 loss: 0.0036209580704297233\n",
      "SMA 250 loss: -0.0019140259976067194\n",
      "SMA 250 entropy: 1.2774464235073182\n",
      "FPS: 280\n",
      "ETA: 4:44:27\n",
      "Max reward: 15.0\n",
      "Number of games: 1179\n",
      "SOFTMAX:  tensor([0.3326, 0.2237, 0.2640, 0.1797], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 210 / 4882 ----------\n",
      "SMA 50 Length: 212.78\n",
      "SMA 50 Reward: 2.04\n",
      "SMA 250 pgloss: 0.008029363689933062\n",
      "SMA 250 vloss: 0.2080980756807895\n",
      "SMA 250 ae1 loss: 0.002664224877731786\n",
      "SMA 250 ae2 loss: 0.0036109569393807934\n",
      "SMA 250 loss: -0.004768389392466772\n",
      "SMA 250 entropy: 1.2797753481637864\n",
      "FPS: 282\n",
      "ETA: 4:42:22\n",
      "Max reward: 15.0\n",
      "Number of games: 1207\n",
      "SOFTMAX:  tensor([0.2779, 0.2296, 0.2827, 0.2097], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 215 / 4882 ----------\n",
      "SMA 50 Length: 197.14\n",
      "SMA 50 Reward: 1.7\n",
      "SMA 250 pgloss: 0.006872783198162134\n",
      "SMA 250 vloss: 0.2073874777486158\n",
      "SMA 250 ae1 loss: 0.0026618732865033453\n",
      "SMA 250 ae2 loss: 0.0036084396309804083\n",
      "SMA 250 loss: -0.00594856456615204\n",
      "SMA 250 entropy: 1.282134815703991\n",
      "FPS: 284\n",
      "ETA: 4:40:23\n",
      "Max reward: 15.0\n",
      "Number of games: 1237\n",
      "SOFTMAX:  tensor([0.2599, 0.2379, 0.2811, 0.2210], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 220 / 4882 ----------\n",
      "SMA 50 Length: 179.52\n",
      "SMA 50 Reward: 1.22\n",
      "SMA 250 pgloss: 0.006367663827206178\n",
      "SMA 250 vloss: 0.20666776116598737\n",
      "SMA 250 ae1 loss: 0.0026620877896096897\n",
      "SMA 250 ae2 loss: 0.003609333572570573\n",
      "SMA 250 loss: -0.006476167809556831\n",
      "SMA 250 entropy: 1.2843831999735398\n",
      "FPS: 285\n",
      "ETA: 4:38:31\n",
      "Max reward: 15.0\n",
      "Number of games: 1269\n",
      "SOFTMAX:  tensor([0.2634, 0.2346, 0.2880, 0.2140], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 225 / 4882 ----------\n",
      "SMA 50 Length: 178.7\n",
      "SMA 50 Reward: 1.26\n",
      "SMA 250 pgloss: 0.007663412460177723\n",
      "SMA 250 vloss: 0.20764545546637642\n",
      "SMA 250 ae1 loss: 0.0026537902037509615\n",
      "SMA 250 ae2 loss: 0.003598724390483565\n",
      "SMA 250 loss: -0.005201075010829502\n",
      "SMA 250 entropy: 1.286448776457045\n",
      "FPS: 287\n",
      "ETA: 4:36:49\n",
      "Max reward: 15.0\n",
      "Number of games: 1295\n",
      "SOFTMAX:  tensor([0.2649, 0.2332, 0.2944, 0.2075], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 230 / 4882 ----------\n",
      "SMA 50 Length: 195.74\n",
      "SMA 50 Reward: 1.76\n",
      "SMA 250 pgloss: 0.007114257426355439\n",
      "SMA 250 vloss: 0.20770705245111298\n",
      "SMA 250 ae1 loss: 0.0026508012528369284\n",
      "SMA 250 ae2 loss: 0.003597308565741\n",
      "SMA 250 loss: -0.005770525652105393\n",
      "SMA 250 entropy: 1.2884783397550168\n",
      "FPS: 288\n",
      "ETA: 4:35:10\n",
      "Max reward: 15.0\n",
      "Number of games: 1326\n",
      "SOFTMAX:  tensor([0.2562, 0.2355, 0.2889, 0.2194], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 235 / 4882 ----------\n",
      "SMA 50 Length: 188.98\n",
      "SMA 50 Reward: 1.6\n",
      "SMA 250 pgloss: 0.00677237350350008\n",
      "SMA 250 vloss: 0.20873907087965216\n",
      "SMA 250 ae1 loss: 0.002646398465545412\n",
      "SMA 250 ae2 loss: 0.003591927880064604\n",
      "SMA 250 loss: -0.006131797434484705\n",
      "SMA 250 entropy: 1.2904171197972398\n",
      "FPS: 290\n",
      "ETA: 4:33:29\n",
      "Max reward: 15.0\n",
      "Number of games: 1354\n",
      "SOFTMAX:  tensor([0.2535, 0.2308, 0.3013, 0.2144], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 240 / 4882 ----------\n",
      "SMA 50 Length: 186.7\n",
      "SMA 50 Reward: 1.48\n",
      "SMA 250 pgloss: 0.007293082546433046\n",
      "SMA 250 vloss: 0.2081581690038244\n",
      "SMA 250 ae1 loss: 0.0026426954684817855\n",
      "SMA 250 ae2 loss: 0.0035869130874440695\n",
      "SMA 250 loss: -0.005628771812189371\n",
      "SMA 250 entropy: 1.2921854610244432\n",
      "FPS: 291\n",
      "ETA: 4:31:54\n",
      "Max reward: 15.0\n",
      "Number of games: 1385\n",
      "SOFTMAX:  tensor([0.2468, 0.2275, 0.3158, 0.2099], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 245 / 4882 ----------\n",
      "SMA 50 Length: 202.08\n",
      "SMA 50 Reward: 2.04\n",
      "SMA 250 pgloss: 0.007014582889411142\n",
      "SMA 250 vloss: 0.21013168881134112\n",
      "SMA 250 ae1 loss: 0.002633174881339073\n",
      "SMA 250 ae2 loss: 0.0035724063812545975\n",
      "SMA 250 loss: -0.005924053227871048\n",
      "SMA 250 entropy: 1.293863650730678\n",
      "FPS: 292\n",
      "ETA: 4:30:22\n",
      "Max reward: 15.0\n",
      "Number of games: 1408\n",
      "SOFTMAX:  tensor([0.2519, 0.2294, 0.3054, 0.2133], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 250 / 4882 ----------\n",
      "SMA 50 Length: 196.88\n",
      "SMA 50 Reward: 2.0\n",
      "SMA 250 pgloss: 0.004620114234974608\n",
      "SMA 250 vloss: 0.20981479227542876\n",
      "SMA 250 ae1 loss: 0.0026352405101060868\n",
      "SMA 250 ae2 loss: 0.0035742036532610657\n",
      "SMA 250 loss: -0.00833555607125163\n",
      "SMA 250 entropy: 1.295567063808441\n",
      "FPS: 294\n",
      "ETA: 4:28:53\n",
      "Max reward: 15.0\n",
      "Number of games: 1443\n",
      "SOFTMAX:  tensor([0.2535, 0.2302, 0.2984, 0.2178], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 255 / 4882 ----------\n",
      "SMA 50 Length: 178.28\n",
      "SMA 50 Reward: 1.3\n",
      "SMA 250 pgloss: 0.004285936740925536\n",
      "SMA 250 vloss: 0.21155574077367784\n",
      "SMA 250 ae1 loss: 0.0025303963562473657\n",
      "SMA 250 ae2 loss: 0.003472298413515091\n",
      "SMA 250 loss: -0.008762413676828146\n",
      "SMA 250 entropy: 1.3048350763320924\n",
      "FPS: 295\n",
      "ETA: 4:27:25\n",
      "Max reward: 15.0\n",
      "Number of games: 1472\n",
      "SOFTMAX:  tensor([0.2535, 0.2330, 0.2927, 0.2208], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 260 / 4882 ----------\n",
      "SMA 50 Length: 204.46\n",
      "SMA 50 Reward: 1.94\n",
      "SMA 250 pgloss: 0.007099825036944821\n",
      "SMA 250 vloss: 0.21387454944849013\n",
      "SMA 250 ae1 loss: 0.0025082333576865496\n",
      "SMA 250 ae2 loss: 0.003452110600657761\n",
      "SMA 250 loss: -0.0060783001165837045\n",
      "SMA 250 entropy: 1.3178125491142274\n",
      "FPS: 296\n",
      "ETA: 4:26:00\n",
      "Max reward: 15.0\n",
      "Number of games: 1496\n",
      "SOFTMAX:  tensor([0.2578, 0.2292, 0.3038, 0.2092], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 265 / 4882 ----------\n",
      "SMA 50 Length: 208.6\n",
      "SMA 50 Reward: 2.08\n",
      "SMA 250 pgloss: 0.006241002598078921\n",
      "SMA 250 vloss: 0.2143121482729912\n",
      "SMA 250 ae1 loss: 0.002494548797607422\n",
      "SMA 250 ae2 loss: 0.003442107428796589\n",
      "SMA 250 loss: -0.007024424595758319\n",
      "SMA 250 entropy: 1.3265427508354186\n",
      "FPS: 297\n",
      "ETA: 4:24:40\n",
      "Max reward: 15.0\n",
      "Number of games: 1523\n",
      "SOFTMAX:  tensor([0.2562, 0.2356, 0.2949, 0.2133], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "---------- 270 / 4882 ----------\n",
      "SMA 50 Length: 186.96\n",
      "SMA 50 Reward: 1.52\n",
      "SMA 250 pgloss: 0.005254960800753906\n",
      "SMA 250 vloss: 0.21484748178720475\n",
      "SMA 250 ae1 loss: 0.002491848753765225\n",
      "SMA 250 ae2 loss: 0.0034405340161174536\n",
      "SMA 250 loss: -0.008029555289074779\n",
      "SMA 250 entropy: 1.3284516415596008\n",
      "FPS: 298\n",
      "ETA: 4:23:22\n",
      "Max reward: 15.0\n",
      "Number of games: 1553\n",
      "SOFTMAX:  tensor([0.2501, 0.2365, 0.3048, 0.2087], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n",
      "Doing 3 repr updates...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9fc55a8e29f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_UPDATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_d1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_d2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# do 100 updates on representation networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-77aab084e6b1>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mnew_ob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_ob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \"\"\"\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/common/vec_env/vec_frame_stack.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mlast_ax_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstackedobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstackedobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlast_ax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;31m# save final observation where user can get it, then reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'terminal_observation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         return (self._obs_from_buf(), np.copy(self.buf_rews), np.copy(self.buf_dones),\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRewardWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mObservationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/common/atari_wrappers.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/common/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/bench/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/common/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/common/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for update in range(1, NUM_UPDATES):\n",
    "\n",
    "    obs, obs_d1, obs_d2, logps, actions, values, rewards, infos = next(rgen)\n",
    "\n",
    "    # do 100 updates on representation networks\n",
    "    num_repr = max(50-update, 3)\n",
    "    print(f'Doing {num_repr} repr updates...')\n",
    "    for _ in range(num_repr):\n",
    "        # update representation network S_{t+1}\n",
    "        pred_ae_1, z1 = ae_delta_1(obs, actions.cpu().numpy().astype(int))\n",
    "        citerion  = nn.MSELoss()\n",
    "        loss_ae_1 = citerion(pred_ae_1, obs_d1)    # UNET + DIFFERENCE (not absolute prediction)\n",
    "        ae_optim_1.zero_grad()\n",
    "        loss_ae_1.backward()\n",
    "        nn.utils.clip_grad_norm_(ae_delta_1.parameters(), 0.5)\n",
    "        ae_optim_1.step()\n",
    "\n",
    "        # update representation network S_{t+2}\n",
    "        pred_ae_2, z2 = ae_delta_2(obs, actions.cpu().numpy().astype(int))\n",
    "        citerion  = nn.MSELoss()\n",
    "        loss_ae_2 = citerion(pred_ae_2, obs_d2)\n",
    "        ae_optim_2.zero_grad()\n",
    "        loss_ae_2.backward()\n",
    "        nn.utils.clip_grad_norm_(ae_delta_2.parameters(), 0.5)\n",
    "        ae_optim_2.step()\n",
    "\n",
    "        # update critic\n",
    "        z_join  = T.cat((z1.detach(), z2.detach()), dim=1)\n",
    "        v = critic(z_join)\n",
    "        v_loss  = ((v - rewards)**2).mean()\n",
    "        critic_optim.zero_grad()\n",
    "        v_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(critic.parameters(), 0.5)\n",
    "        critic_optim.step()\n",
    "\n",
    "    # update actor\n",
    "    adv = rewards - v.detach()\n",
    "    # adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "    # get new latent representations\n",
    "    z1 = ae_delta_1.encode(obs).detach()\n",
    "    z2 = ae_delta_2.encode(obs).detach()\n",
    "\n",
    "    # infer behaviour from learned representation of env\n",
    "    z_join  = T.cat((z1, z2), dim=1)\n",
    "    pi      = actor(z_join)\n",
    "    dist    = Categorical(logits=pi)\n",
    "    logps   = dist.log_prob(actions)\n",
    "    \n",
    "    # losses\n",
    "    pg_loss = -(logps * adv).mean()\n",
    "    entropy = dist.entropy().mean()\n",
    "    loss = pg_loss - 0.01 * entropy\n",
    "    actor_optim.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(actor.parameters(), 0.5)\n",
    "    actor_optim.step()\n",
    "    \n",
    "    # decrease learning rates linearly\n",
    "    actor_scheduler.step(update)\n",
    "    critic_scheduler.step(update)\n",
    "    ae_1_scheduler.step(update)\n",
    "    ae_2_scheduler.step(update)\n",
    "\n",
    "    # log stats\n",
    "    logger.update(\n",
    "        infos, pg_loss.item(), v_loss.item(), \n",
    "        entropy.item(), loss.item(), \n",
    "        loss_ae_1.item(), loss_ae_2.item(), \n",
    "        print_rate=5\n",
    "    )\n",
    "    if update%5 == 0:\n",
    "        print('SOFTMAX: ', dist.probs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 965,
     "status": "ok",
     "timestamp": 1582314310243,
     "user": {
      "displayName": "harry songhurst",
      "photoUrl": "",
      "userId": "00629498674930426914"
     },
     "user_tz": 0
    },
    "id": "ZVrqNT8s1Kyk",
    "outputId": "b034a755-2ea0-4e67-cb43-0558ef118368"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f425a0e47b8>"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nelRMa2FqkxYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(pred_ae_1.detach().cpu().numpy()[590][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CT66MfX-AHt1",
    "outputId": "039e1d37-eb3a-47b4-d4da-6441878504ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 5 / 976 ----------\n",
      "SMA 50 Length: 199.21875\n",
      "SMA 50 Reward: 2.0\n",
      "SMA 250 pgloss: -0.005999824864557013\n",
      "SMA 250 vloss: 0.08127210550010204\n",
      "SMA 250 intrinsic: 0.004866595193743706\n",
      "SMA 250 loss: 0.010406126081943513\n",
      "SMA 250 entropy: 1.211505126953125\n",
      "FPS: 334\n",
      "ETA: 0:49:29\n",
      "Max reward: 14.0\n",
      "Number of games: 32\n",
      "SOFTMAX:  tensor([0.1200, 0.1907, 0.4588, 0.2306], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 10 / 976 ----------\n",
      "SMA 50 Length: 194.08\n",
      "SMA 50 Reward: 1.78\n",
      "SMA 250 pgloss: -0.00422924869635608\n",
      "SMA 250 vloss: 0.09672766830772161\n",
      "SMA 250 intrinsic: 0.004486960940994322\n",
      "SMA 250 loss: 0.01947435149922967\n",
      "SMA 250 entropy: 1.2330117702484131\n",
      "FPS: 344\n",
      "ETA: 0:47:51\n",
      "Max reward: 14.0\n",
      "Number of games: 59\n",
      "SOFTMAX:  tensor([0.1625, 0.2177, 0.3756, 0.2443], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 15 / 976 ----------\n",
      "SMA 50 Length: 189.38\n",
      "SMA 50 Reward: 1.52\n",
      "SMA 250 pgloss: -0.004090286040445789\n",
      "SMA 250 vloss: 0.10791716116170089\n",
      "SMA 250 intrinsic: 0.00455971226717035\n",
      "SMA 250 loss: 0.02487414249529441\n",
      "SMA 250 entropy: 1.2497076352437337\n",
      "FPS: 345\n",
      "ETA: 0:47:32\n",
      "Max reward: 14.0\n",
      "Number of games: 91\n",
      "SOFTMAX:  tensor([0.0705, 0.1543, 0.4240, 0.3512], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 20 / 976 ----------\n",
      "SMA 50 Length: 189.38\n",
      "SMA 50 Reward: 1.54\n",
      "SMA 250 pgloss: -0.0034004380271653646\n",
      "SMA 250 vloss: 0.11726504964753985\n",
      "SMA 250 intrinsic: 0.00438751281471923\n",
      "SMA 250 loss: 0.030286346888169648\n",
      "SMA 250 entropy: 1.2472870111465455\n",
      "FPS: 345\n",
      "ETA: 0:47:17\n",
      "Max reward: 14.0\n",
      "Number of games: 116\n",
      "SOFTMAX:  tensor([0.0896, 0.1515, 0.4788, 0.2801], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 25 / 976 ----------\n",
      "SMA 50 Length: 216.2\n",
      "SMA 50 Reward: 2.32\n",
      "SMA 250 pgloss: -0.0028525467321742328\n",
      "SMA 250 vloss: 0.12745746783912182\n",
      "SMA 250 intrinsic: 0.004395293304696679\n",
      "SMA 250 loss: 0.03587738718837499\n",
      "SMA 250 entropy: 1.2499400329589845\n",
      "FPS: 347\n",
      "ETA: 0:46:46\n",
      "Max reward: 14.0\n",
      "Number of games: 140\n",
      "SOFTMAX:  tensor([0.0717, 0.1642, 0.4761, 0.2880], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 30 / 976 ----------\n",
      "SMA 50 Length: 201.84\n",
      "SMA 50 Reward: 1.96\n",
      "SMA 250 pgloss: -0.0023734121403928537\n",
      "SMA 250 vloss: 0.12816933076828718\n",
      "SMA 250 intrinsic: 0.004379191203042865\n",
      "SMA 250 loss: 0.03659724565222859\n",
      "SMA 250 entropy: 1.25570041735967\n",
      "FPS: 347\n",
      "ETA: 0:46:27\n",
      "Max reward: 14.0\n",
      "Number of games: 170\n",
      "SOFTMAX:  tensor([0.1340, 0.2579, 0.3373, 0.2708], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 35 / 976 ----------\n",
      "SMA 50 Length: 196.58\n",
      "SMA 50 Reward: 1.74\n",
      "SMA 250 pgloss: -0.0023456856537710075\n",
      "SMA 250 vloss: 0.13552808362458432\n",
      "SMA 250 intrinsic: 0.004297163876305733\n",
      "SMA 250 loss: 0.040170710400811264\n",
      "SMA 250 entropy: 1.262382343837193\n",
      "FPS: 348\n",
      "ETA: 0:46:09\n",
      "Max reward: 14.0\n",
      "Number of games: 194\n",
      "SOFTMAX:  tensor([0.1423, 0.2202, 0.3904, 0.2471], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 40 / 976 ----------\n",
      "SMA 50 Length: 218.44\n",
      "SMA 50 Reward: 2.24\n",
      "SMA 250 pgloss: -0.002241683033571462\n",
      "SMA 250 vloss: 0.14575224039144813\n",
      "SMA 250 intrinsic: 0.004228333086939528\n",
      "SMA 250 loss: 0.04527247652877122\n",
      "SMA 250 entropy: 1.2680980920791627\n",
      "FPS: 347\n",
      "ETA: 0:45:57\n",
      "Max reward: 14.0\n",
      "Number of games: 221\n",
      "SOFTMAX:  tensor([0.1596, 0.2411, 0.3818, 0.2174], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 45 / 976 ----------\n",
      "SMA 50 Length: 220.08\n",
      "SMA 50 Reward: 2.32\n",
      "SMA 250 pgloss: -0.0019389181420491595\n",
      "SMA 250 vloss: 0.15080419584280916\n",
      "SMA 250 intrinsic: 0.004147460177126858\n",
      "SMA 250 loss: 0.04797879769984219\n",
      "SMA 250 entropy: 1.2742191685570612\n",
      "FPS: 347\n",
      "ETA: 0:45:42\n",
      "Max reward: 14.0\n",
      "Number of games: 246\n",
      "SOFTMAX:  tensor([0.1399, 0.2423, 0.3985, 0.2193], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 50 / 976 ----------\n",
      "SMA 50 Length: 212.32\n",
      "SMA 50 Reward: 2.16\n",
      "SMA 250 pgloss: -0.0018845245914417318\n",
      "SMA 250 vloss: 0.15015727285295724\n",
      "SMA 250 intrinsic: 0.004142435314133763\n",
      "SMA 250 loss: 0.04763694053515792\n",
      "SMA 250 entropy: 1.2778586196899413\n",
      "FPS: 348\n",
      "ETA: 0:45:21\n",
      "Max reward: 14.0\n",
      "Number of games: 273\n",
      "SOFTMAX:  tensor([0.1197, 0.2199, 0.4372, 0.2233], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 55 / 976 ----------\n",
      "SMA 50 Length: 203.56\n",
      "SMA 50 Reward: 1.98\n",
      "SMA 250 pgloss: -0.001877896780074066\n",
      "SMA 250 vloss: 0.14899132383817976\n",
      "SMA 250 intrinsic: 0.0041371580297974025\n",
      "SMA 250 loss: 0.046968990174884144\n",
      "SMA 250 entropy: 1.2824387962167914\n",
      "FPS: 348\n",
      "ETA: 0:45:04\n",
      "Max reward: 14.0\n",
      "Number of games: 301\n",
      "SOFTMAX:  tensor([0.1685, 0.2866, 0.3444, 0.2005], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 60 / 976 ----------\n",
      "SMA 50 Length: 195.24\n",
      "SMA 50 Reward: 1.76\n",
      "SMA 250 pgloss: -0.002389835640254508\n",
      "SMA 250 vloss: 0.14993820001060765\n",
      "SMA 250 intrinsic: 0.004128736288597186\n",
      "SMA 250 loss: 0.04689688861059646\n",
      "SMA 250 entropy: 1.284118833144506\n",
      "FPS: 349\n",
      "ETA: 0:44:48\n",
      "Max reward: 14.0\n",
      "Number of games: 329\n",
      "SOFTMAX:  tensor([0.1381, 0.1838, 0.5109, 0.1671], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 65 / 976 ----------\n",
      "SMA 50 Length: 193.52\n",
      "SMA 50 Reward: 1.78\n",
      "SMA 250 pgloss: -0.0023669023263769655\n",
      "SMA 250 vloss: 0.150611509468693\n",
      "SMA 250 intrinsic: 0.004103961970227269\n",
      "SMA 250 loss: 0.04718983412648623\n",
      "SMA 250 entropy: 1.2874509481283334\n",
      "FPS: 349\n",
      "ETA: 0:44:33\n",
      "Max reward: 14.0\n",
      "Number of games: 358\n",
      "SOFTMAX:  tensor([0.2191, 0.2304, 0.3311, 0.2193], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 70 / 976 ----------\n",
      "SMA 50 Length: 183.14\n",
      "SMA 50 Reward: 1.38\n",
      "SMA 250 pgloss: -0.0026802324739816996\n",
      "SMA 250 vloss: 0.14891991676496608\n",
      "SMA 250 intrinsic: 0.004069857631943056\n",
      "SMA 250 loss: 0.04609301985640611\n",
      "SMA 250 entropy: 1.284335333960397\n",
      "FPS: 349\n",
      "ETA: 0:44:15\n",
      "Max reward: 14.0\n",
      "Number of games: 388\n",
      "SOFTMAX:  tensor([0.0853, 0.1625, 0.6207, 0.1315], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 75 / 976 ----------\n",
      "SMA 50 Length: 201.58\n",
      "SMA 50 Reward: 1.94\n",
      "SMA 250 pgloss: -0.003127503890233735\n",
      "SMA 250 vloss: 0.14948999511698882\n",
      "SMA 250 intrinsic: 0.004009358414138357\n",
      "SMA 250 loss: 0.04603075639655193\n",
      "SMA 250 entropy: 1.2793368927637736\n",
      "FPS: 349\n",
      "ETA: 0:43:59\n",
      "Max reward: 14.0\n",
      "Number of games: 412\n",
      "SOFTMAX:  tensor([0.2316, 0.2421, 0.3180, 0.2083], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 80 / 976 ----------\n",
      "SMA 50 Length: 214.76\n",
      "SMA 50 Reward: 2.34\n",
      "SMA 250 pgloss: -0.002990019563367241\n",
      "SMA 250 vloss: 0.15246787869837136\n",
      "SMA 250 intrinsic: 0.003970751547603868\n",
      "SMA 250 loss: 0.047650727781001476\n",
      "SMA 250 entropy: 1.279659628868103\n",
      "FPS: 350\n",
      "ETA: 0:43:42\n",
      "Max reward: 14.0\n",
      "Number of games: 440\n",
      "SOFTMAX:  tensor([0.1497, 0.2002, 0.4729, 0.1772], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 85 / 976 ----------\n",
      "SMA 50 Length: 203.48\n",
      "SMA 50 Reward: 1.98\n",
      "SMA 250 pgloss: -0.002924033109550638\n",
      "SMA 250 vloss: 0.1513278531458448\n",
      "SMA 250 intrinsic: 0.0039520953973645676\n",
      "SMA 250 loss: 0.04710999355815789\n",
      "SMA 250 entropy: 1.281495029786054\n",
      "FPS: 350\n",
      "ETA: 0:43:27\n",
      "Max reward: 14.0\n",
      "Number of games: 468\n",
      "SOFTMAX:  tensor([0.2046, 0.2412, 0.3502, 0.2040], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 90 / 976 ----------\n",
      "SMA 50 Length: 206.38\n",
      "SMA 50 Reward: 2.1\n",
      "SMA 250 pgloss: -0.002786141572707695\n",
      "SMA 250 vloss: 0.15543410362054905\n",
      "SMA 250 intrinsic: 0.0038903563593824706\n",
      "SMA 250 loss: 0.04931693628637327\n",
      "SMA 250 entropy: 1.2806987404823302\n",
      "FPS: 350\n",
      "ETA: 0:43:09\n",
      "Max reward: 14.0\n",
      "Number of games: 490\n",
      "SOFTMAX:  tensor([0.1924, 0.2490, 0.3395, 0.2191], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 95 / 976 ----------\n",
      "SMA 50 Length: 218.6\n",
      "SMA 50 Reward: 2.42\n",
      "SMA 250 pgloss: -0.0028323112345165816\n",
      "SMA 250 vloss: 0.15660426583337156\n",
      "SMA 250 intrinsic: 0.003853116260449353\n",
      "SMA 250 loss: 0.049940633528718824\n",
      "SMA 250 entropy: 1.276459452980443\n",
      "FPS: 350\n",
      "ETA: 0:42:55\n",
      "Max reward: 14.0\n",
      "Number of games: 514\n",
      "SOFTMAX:  tensor([0.1240, 0.2111, 0.4413, 0.2236], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 100 / 976 ----------\n",
      "SMA 50 Length: 226.46\n",
      "SMA 50 Reward: 2.62\n",
      "SMA 250 pgloss: -0.003085080802120501\n",
      "SMA 250 vloss: 0.15637537496164441\n",
      "SMA 250 intrinsic: 0.003835498497355729\n",
      "SMA 250 loss: 0.04957075542770326\n",
      "SMA 250 entropy: 1.2765926122665405\n",
      "FPS: 350\n",
      "ETA: 0:42:39\n",
      "Max reward: 14.0\n",
      "Number of games: 539\n",
      "SOFTMAX:  tensor([0.1288, 0.2662, 0.3310, 0.2740], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 105 / 976 ----------\n",
      "SMA 50 Length: 199.18\n",
      "SMA 50 Reward: 1.84\n",
      "SMA 250 pgloss: -0.0030149746504667704\n",
      "SMA 250 vloss: 0.15361341647803783\n",
      "SMA 250 intrinsic: 0.0038431667988853796\n",
      "SMA 250 loss: 0.048256774557133515\n",
      "SMA 250 entropy: 1.2767479998724802\n",
      "FPS: 350\n",
      "ETA: 0:42:27\n",
      "Max reward: 14.0\n",
      "Number of games: 571\n",
      "SOFTMAX:  tensor([0.1433, 0.2497, 0.3788, 0.2283], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 110 / 976 ----------\n",
      "SMA 50 Length: 179.22\n",
      "SMA 50 Reward: 1.28\n",
      "SMA 250 pgloss: -0.002934258134998593\n",
      "SMA 250 vloss: 0.15398571658879517\n",
      "SMA 250 intrinsic: 0.003812659620730714\n",
      "SMA 250 loss: 0.048571041675115174\n",
      "SMA 250 entropy: 1.2743779702620073\n",
      "FPS: 350\n",
      "ETA: 0:42:11\n",
      "Max reward: 14.0\n",
      "Number of games: 597\n",
      "SOFTMAX:  tensor([0.1208, 0.3146, 0.3464, 0.2181], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 115 / 976 ----------\n",
      "SMA 50 Length: 196.68\n",
      "SMA 50 Reward: 1.78\n",
      "SMA 250 pgloss: -0.0027590176193094205\n",
      "SMA 250 vloss: 0.15579965112973815\n",
      "SMA 250 intrinsic: 0.003802461772347274\n",
      "SMA 250 loss: 0.0496698089186912\n",
      "SMA 250 entropy: 1.2735499972882478\n",
      "FPS: 350\n",
      "ETA: 0:41:57\n",
      "Max reward: 14.0\n",
      "Number of games: 627\n",
      "SOFTMAX:  tensor([0.1611, 0.2680, 0.3229, 0.2480], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 120 / 976 ----------\n",
      "SMA 50 Length: 168.8\n",
      "SMA 50 Reward: 1.08\n",
      "SMA 250 pgloss: -0.0026856560232166276\n",
      "SMA 250 vloss: 0.15342839383520185\n",
      "SMA 250 intrinsic: 0.0038211310709205764\n",
      "SMA 250 loss: 0.04855003342187653\n",
      "SMA 250 entropy: 1.2739254196484884\n",
      "FPS: 350\n",
      "ETA: 0:41:45\n",
      "Max reward: 14.0\n",
      "Number of games: 660\n",
      "SOFTMAX:  tensor([0.0692, 0.3615, 0.3592, 0.2101], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 125 / 976 ----------\n",
      "SMA 50 Length: 175.06\n",
      "SMA 50 Reward: 1.28\n",
      "SMA 250 pgloss: -0.0026363488420611246\n",
      "SMA 250 vloss: 0.15230689923465252\n",
      "SMA 250 intrinsic: 0.0038326048720628025\n",
      "SMA 250 loss: 0.04801015041023493\n",
      "SMA 250 entropy: 1.2753475637435914\n",
      "FPS: 349\n",
      "ETA: 0:41:37\n",
      "Max reward: 14.0\n",
      "Number of games: 692\n",
      "SOFTMAX:  tensor([0.1533, 0.3051, 0.2803, 0.2613], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 130 / 976 ----------\n",
      "SMA 50 Length: 179.58\n",
      "SMA 50 Reward: 1.4\n",
      "SMA 250 pgloss: -0.0025579654091136316\n",
      "SMA 250 vloss: 0.1539401347677295\n",
      "SMA 250 intrinsic: 0.003831170701708358\n",
      "SMA 250 loss: 0.048903735089474\n",
      "SMA 250 entropy: 1.2754183888435364\n",
      "FPS: 349\n",
      "ETA: 0:41:23\n",
      "Max reward: 14.0\n",
      "Number of games: 721\n",
      "SOFTMAX:  tensor([0.0882, 0.3935, 0.3068, 0.2115], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 135 / 976 ----------\n",
      "SMA 50 Length: 203.14\n",
      "SMA 50 Reward: 1.92\n",
      "SMA 250 pgloss: -0.0025186115792840374\n",
      "SMA 250 vloss: 0.155706679917596\n",
      "SMA 250 intrinsic: 0.00381301859807637\n",
      "SMA 250 loss: 0.0498144496301258\n",
      "SMA 250 entropy: 1.2760139774393153\n",
      "FPS: 348\n",
      "ETA: 0:41:09\n",
      "Max reward: 14.0\n",
      "Number of games: 745\n",
      "SOFTMAX:  tensor([0.1113, 0.3382, 0.3137, 0.2369], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 140 / 976 ----------\n",
      "SMA 50 Length: 226.04\n",
      "SMA 50 Reward: 2.44\n",
      "SMA 250 pgloss: -0.0024401115465609888\n",
      "SMA 250 vloss: 0.15646806858213885\n",
      "SMA 250 intrinsic: 0.0037883363622573337\n",
      "SMA 250 loss: 0.050259077262931634\n",
      "SMA 250 entropy: 1.2767423178468431\n",
      "FPS: 349\n",
      "ETA: 0:40:54\n",
      "Max reward: 14.0\n",
      "Number of games: 769\n",
      "SOFTMAX:  tensor([0.1604, 0.2813, 0.2782, 0.2801], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 145 / 976 ----------\n",
      "SMA 50 Length: 214.42\n",
      "SMA 50 Reward: 2.2\n",
      "SMA 250 pgloss: -0.0023004071632385602\n",
      "SMA 250 vloss: 0.1567909340791661\n",
      "SMA 250 intrinsic: 0.0038043439139387215\n",
      "SMA 250 loss: 0.050550978007758485\n",
      "SMA 250 entropy: 1.2772041378350094\n",
      "FPS: 348\n",
      "ETA: 0:40:42\n",
      "Max reward: 14.0\n",
      "Number of games: 798\n",
      "SOFTMAX:  tensor([0.0988, 0.1641, 0.5378, 0.1993], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 150 / 976 ----------\n",
      "SMA 50 Length: 186.5\n",
      "SMA 50 Reward: 1.56\n",
      "SMA 250 pgloss: -0.0022198364749723017\n",
      "SMA 250 vloss: 0.15614186231046914\n",
      "SMA 250 intrinsic: 0.003797004248481244\n",
      "SMA 250 loss: 0.050299202209959426\n",
      "SMA 250 entropy: 1.2775946648915608\n",
      "FPS: 348\n",
      "ETA: 0:40:30\n",
      "Max reward: 14.0\n",
      "Number of games: 828\n",
      "SOFTMAX:  tensor([0.1400, 0.2917, 0.2950, 0.2733], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 155 / 976 ----------\n",
      "SMA 50 Length: 178.62\n",
      "SMA 50 Reward: 1.26\n",
      "SMA 250 pgloss: -0.0022334419203029793\n",
      "SMA 250 vloss: 0.1580190310434949\n",
      "SMA 250 intrinsic: 0.0037900313637369582\n",
      "SMA 250 loss: 0.05120507159660901\n",
      "SMA 250 entropy: 1.2785501372429633\n",
      "FPS: 347\n",
      "ETA: 0:40:18\n",
      "Max reward: 14.0\n",
      "Number of games: 858\n",
      "SOFTMAX:  tensor([0.1298, 0.3147, 0.3012, 0.2543], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 160 / 976 ----------\n",
      "SMA 50 Length: 195.34\n",
      "SMA 50 Reward: 1.76\n",
      "SMA 250 pgloss: -0.002210204499436941\n",
      "SMA 250 vloss: 0.1557388012879528\n",
      "SMA 250 intrinsic: 0.0038071009497798514\n",
      "SMA 250 loss: 0.05005821593222208\n",
      "SMA 250 entropy: 1.2800490468740464\n",
      "FPS: 347\n",
      "ETA: 0:40:08\n",
      "Max reward: 14.0\n",
      "Number of games: 887\n",
      "SOFTMAX:  tensor([0.1006, 0.3949, 0.3010, 0.2034], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 165 / 976 ----------\n",
      "SMA 50 Length: 185.92\n",
      "SMA 50 Reward: 1.54\n",
      "SMA 250 pgloss: -0.0020483065704287253\n",
      "SMA 250 vloss: 0.15572877816404357\n",
      "SMA 250 intrinsic: 0.003808410103771497\n",
      "SMA 250 loss: 0.05020691146791884\n",
      "SMA 250 entropy: 1.280458587588686\n",
      "FPS: 346\n",
      "ETA: 0:39:56\n",
      "Max reward: 14.0\n",
      "Number of games: 916\n",
      "SOFTMAX:  tensor([0.1354, 0.3777, 0.2518, 0.2351], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 170 / 976 ----------\n",
      "SMA 50 Length: 189.74\n",
      "SMA 50 Reward: 1.66\n",
      "SMA 250 pgloss: -0.002115188221363068\n",
      "SMA 250 vloss: 0.155838779108051\n",
      "SMA 250 intrinsic: 0.003798525729079676\n",
      "SMA 250 loss: 0.050209071240661776\n",
      "SMA 250 entropy: 1.2797565376057345\n",
      "FPS: 346\n",
      "ETA: 0:39:43\n",
      "Max reward: 14.0\n",
      "Number of games: 944\n",
      "SOFTMAX:  tensor([0.0659, 0.4326, 0.3469, 0.1545], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 175 / 976 ----------\n",
      "SMA 50 Length: 210.92\n",
      "SMA 50 Reward: 2.24\n",
      "SMA 250 pgloss: -0.0022301196613729447\n",
      "SMA 250 vloss: 0.15446298204362394\n",
      "SMA 250 intrinsic: 0.0037962238949590496\n",
      "SMA 250 loss: 0.04944317810769592\n",
      "SMA 250 entropy: 1.2779096950803484\n",
      "FPS: 346\n",
      "ETA: 0:39:30\n",
      "Max reward: 14.0\n",
      "Number of games: 969\n",
      "SOFTMAX:  tensor([0.1219, 0.4233, 0.2336, 0.2212], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 180 / 976 ----------\n",
      "SMA 50 Length: 202.86\n",
      "SMA 50 Reward: 1.98\n",
      "SMA 250 pgloss: -0.002259870546302207\n",
      "SMA 250 vloss: 0.15348666257535418\n",
      "SMA 250 intrinsic: 0.003800785132140542\n",
      "SMA 250 loss: 0.04898225069563422\n",
      "SMA 250 entropy: 1.2750605344772339\n",
      "FPS: 346\n",
      "ETA: 0:39:17\n",
      "Max reward: 14.0\n",
      "Number of games: 999\n",
      "SOFTMAX:  tensor([0.0267, 0.5931, 0.2366, 0.1436], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 185 / 976 ----------\n",
      "SMA 50 Length: 202.24\n",
      "SMA 50 Reward: 1.88\n",
      "SMA 250 pgloss: -0.0021258967621769122\n",
      "SMA 250 vloss: 0.15256757823800718\n",
      "SMA 250 intrinsic: 0.0037904869934946702\n",
      "SMA 250 loss: 0.04869337546563632\n",
      "SMA 250 entropy: 1.2732258783804404\n",
      "FPS: 345\n",
      "ETA: 0:39:02\n",
      "Max reward: 14.0\n",
      "Number of games: 1022\n",
      "SOFTMAX:  tensor([0.0588, 0.4309, 0.3048, 0.2056], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 190 / 976 ----------\n",
      "SMA 50 Length: 204.92\n",
      "SMA 50 Reward: 1.9\n",
      "SMA 250 pgloss: -0.0019603910630729316\n",
      "SMA 250 vloss: 0.1524441479754291\n",
      "SMA 250 intrinsic: 0.0037750239428868027\n",
      "SMA 250 loss: 0.04882519744630707\n",
      "SMA 250 entropy: 1.2718243090729964\n",
      "FPS: 346\n",
      "ETA: 0:38:47\n",
      "Max reward: 14.0\n",
      "Number of games: 1049\n",
      "SOFTMAX:  tensor([0.0783, 0.3918, 0.3088, 0.2211], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 195 / 976 ----------\n",
      "SMA 50 Length: 208.74\n",
      "SMA 50 Reward: 1.98\n",
      "SMA 250 pgloss: -0.0019363991807180216\n",
      "SMA 250 vloss: 0.1523306422222119\n",
      "SMA 250 intrinsic: 0.0037595822875841687\n",
      "SMA 250 loss: 0.04883857099578166\n",
      "SMA 250 entropy: 1.2695175806681316\n",
      "FPS: 345\n",
      "ETA: 0:38:33\n",
      "Max reward: 14.0\n",
      "Number of games: 1075\n",
      "SOFTMAX:  tensor([0.0351, 0.4553, 0.4037, 0.1058], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 200 / 976 ----------\n",
      "SMA 50 Length: 219.6\n",
      "SMA 50 Reward: 2.36\n",
      "SMA 250 pgloss: -0.0018476160353702654\n",
      "SMA 250 vloss: 0.15366937310434878\n",
      "SMA 250 intrinsic: 0.0037389837991213424\n",
      "SMA 250 loss: 0.049646293432451784\n",
      "SMA 250 entropy: 1.2670388877391816\n",
      "FPS: 346\n",
      "ETA: 0:38:17\n",
      "Max reward: 14.0\n",
      "Number of games: 1101\n",
      "SOFTMAX:  tensor([0.0228, 0.5102, 0.3872, 0.0798], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 205 / 976 ----------\n",
      "SMA 50 Length: 224.48\n",
      "SMA 50 Reward: 2.5\n",
      "SMA 250 pgloss: -0.0018798041978321058\n",
      "SMA 250 vloss: 0.1534271478743815\n",
      "SMA 250 intrinsic: 0.003731164028981655\n",
      "SMA 250 loss: 0.04955023677519909\n",
      "SMA 250 entropy: 1.264176680402058\n",
      "FPS: 345\n",
      "ETA: 0:38:03\n",
      "Max reward: 14.0\n",
      "Number of games: 1124\n",
      "SOFTMAX:  tensor([0.0582, 0.4226, 0.3482, 0.1709], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 210 / 976 ----------\n",
      "SMA 50 Length: 219.56\n",
      "SMA 50 Reward: 2.28\n",
      "SMA 250 pgloss: -0.001877002286307418\n",
      "SMA 250 vloss: 0.15284571889787912\n",
      "SMA 250 intrinsic: 0.0037159418947772965\n",
      "SMA 250 loss: 0.04932282932900957\n",
      "SMA 250 entropy: 1.2611514216377622\n",
      "FPS: 346\n",
      "ETA: 0:37:47\n",
      "Max reward: 14.0\n",
      "Number of games: 1150\n",
      "SOFTMAX:  tensor([0.0556, 0.3629, 0.4190, 0.1624], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 215 / 976 ----------\n",
      "SMA 50 Length: 197.78\n",
      "SMA 50 Reward: 1.8\n",
      "SMA 250 pgloss: -0.0018268835690875233\n",
      "SMA 250 vloss: 0.1522037650176952\n",
      "SMA 250 intrinsic: 0.003709438407988569\n",
      "SMA 250 loss: 0.049091114922491616\n",
      "SMA 250 entropy: 1.2591942293699399\n",
      "FPS: 345\n",
      "ETA: 0:37:34\n",
      "Max reward: 14.0\n",
      "Number of games: 1177\n",
      "SOFTMAX:  tensor([0.0934, 0.3713, 0.3079, 0.2274], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 220 / 976 ----------\n",
      "SMA 50 Length: 217.4\n",
      "SMA 50 Reward: 2.28\n",
      "SMA 250 pgloss: -0.0018429750945002028\n",
      "SMA 250 vloss: 0.15222986441274935\n",
      "SMA 250 intrinsic: 0.003683325403306464\n",
      "SMA 250 loss: 0.049141797884790735\n",
      "SMA 250 entropy: 1.2565079900351437\n",
      "FPS: 345\n",
      "ETA: 0:37:19\n",
      "Max reward: 14.0\n",
      "Number of games: 1200\n",
      "SOFTMAX:  tensor([0.0341, 0.3780, 0.4719, 0.1161], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 225 / 976 ----------\n",
      "SMA 50 Length: 226.36\n",
      "SMA 50 Reward: 2.6\n",
      "SMA 250 pgloss: -0.001837436733051921\n",
      "SMA 250 vloss: 0.15137497387826443\n",
      "SMA 250 intrinsic: 0.0036731262541272575\n",
      "SMA 250 loss: 0.04878678144266208\n",
      "SMA 250 entropy: 1.2531634664535523\n",
      "FPS: 345\n",
      "ETA: 0:37:05\n",
      "Max reward: 14.0\n",
      "Number of games: 1227\n",
      "SOFTMAX:  tensor([0.0444, 0.3578, 0.4673, 0.1305], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 230 / 976 ----------\n",
      "SMA 50 Length: 214.0\n",
      "SMA 50 Reward: 2.56\n",
      "SMA 250 pgloss: -0.0018155856273709935\n",
      "SMA 250 vloss: 0.1503499005070847\n",
      "SMA 250 intrinsic: 0.0036659030754969497\n",
      "SMA 250 loss: 0.04834260447760639\n",
      "SMA 250 entropy: 1.2508380345676255\n",
      "FPS: 345\n",
      "ETA: 0:36:50\n",
      "Max reward: 14.0\n",
      "Number of games: 1251\n",
      "SOFTMAX:  tensor([0.0211, 0.3481, 0.5363, 0.0945], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 235 / 976 ----------\n",
      "SMA 50 Length: 229.56\n",
      "SMA 50 Reward: 3.02\n",
      "SMA 250 pgloss: -0.0018717422654554732\n",
      "SMA 250 vloss: 0.14911364054584758\n",
      "SMA 250 intrinsic: 0.003652180935890275\n",
      "SMA 250 loss: 0.04770964995581419\n",
      "SMA 250 entropy: 1.248771428554616\n",
      "FPS: 345\n",
      "ETA: 0:36:35\n",
      "Max reward: 14.0\n",
      "Number of games: 1275\n",
      "SOFTMAX:  tensor([0.0732, 0.4077, 0.3156, 0.2034], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 240 / 976 ----------\n",
      "SMA 50 Length: 214.9\n",
      "SMA 50 Reward: 2.34\n",
      "SMA 250 pgloss: -0.0018840081630022117\n",
      "SMA 250 vloss: 0.1488196408143267\n",
      "SMA 250 intrinsic: 0.003649379477428738\n",
      "SMA 250 loss: 0.047584573664547256\n",
      "SMA 250 entropy: 1.2470619549353918\n",
      "FPS: 346\n",
      "ETA: 0:36:19\n",
      "Max reward: 14.0\n",
      "Number of games: 1305\n",
      "SOFTMAX:  tensor([0.0729, 0.4010, 0.3335, 0.1926], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 245 / 976 ----------\n",
      "SMA 50 Length: 195.88\n",
      "SMA 50 Reward: 1.78\n",
      "SMA 250 pgloss: -0.0017662451117071121\n",
      "SMA 250 vloss: 0.14774480873376739\n",
      "SMA 250 intrinsic: 0.0036445822894611225\n",
      "SMA 250 loss: 0.047190839230862196\n",
      "SMA 250 entropy: 1.245766028092832\n",
      "FPS: 346\n",
      "ETA: 0:36:05\n",
      "Max reward: 14.0\n",
      "Number of games: 1330\n",
      "SOFTMAX:  tensor([0.0984, 0.3842, 0.2859, 0.2315], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 250 / 976 ----------\n",
      "SMA 50 Length: 195.0\n",
      "SMA 50 Reward: 1.74\n",
      "SMA 250 pgloss: -0.0017461372100915468\n",
      "SMA 250 vloss: 0.14721217275410892\n",
      "SMA 250 intrinsic: 0.0036465824753977357\n",
      "SMA 250 loss: 0.04695449918136001\n",
      "SMA 250 entropy: 1.245272527217865\n",
      "FPS: 346\n",
      "ETA: 0:35:49\n",
      "Max reward: 14.0\n",
      "Number of games: 1359\n",
      "SOFTMAX:  tensor([0.1130, 0.3665, 0.2844, 0.2361], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 255 / 976 ----------\n",
      "SMA 50 Length: 189.32\n",
      "SMA 50 Reward: 1.58\n",
      "SMA 250 pgloss: -0.0016948912823972933\n",
      "SMA 250 vloss: 0.1486216644793749\n",
      "SMA 250 intrinsic: 0.003611094185616821\n",
      "SMA 250 loss: 0.047709247183054686\n",
      "SMA 250 entropy: 1.2453347158432007\n",
      "FPS: 346\n",
      "ETA: 0:35:34\n",
      "Max reward: 14.0\n",
      "Number of games: 1388\n",
      "SOFTMAX:  tensor([0.0470, 0.4448, 0.3705, 0.1376], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 260 / 976 ----------\n",
      "SMA 50 Length: 204.24\n",
      "SMA 50 Reward: 1.86\n",
      "SMA 250 pgloss: -0.0016473583128936299\n",
      "SMA 250 vloss: 0.1489969628751278\n",
      "SMA 250 intrinsic: 0.003595036836806685\n",
      "SMA 250 loss: 0.047966809079051015\n",
      "SMA 250 entropy: 1.2442157282829285\n",
      "FPS: 346\n",
      "ETA: 0:35:19\n",
      "Max reward: 14.0\n",
      "Number of games: 1413\n",
      "SOFTMAX:  tensor([0.0406, 0.4411, 0.3896, 0.1287], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 265 / 976 ----------\n",
      "SMA 50 Length: 224.76\n",
      "SMA 50 Reward: 2.34\n",
      "SMA 250 pgloss: -0.0016168923123441345\n",
      "SMA 250 vloss: 0.14850108931958675\n",
      "SMA 250 intrinsic: 0.0035494350926019253\n",
      "SMA 250 loss: 0.047792818337678906\n",
      "SMA 250 entropy: 1.2420417275428772\n",
      "FPS: 346\n",
      "ETA: 0:35:04\n",
      "Max reward: 14.0\n",
      "Number of games: 1433\n",
      "SOFTMAX:  tensor([0.0715, 0.3747, 0.3627, 0.1911], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 270 / 976 ----------\n",
      "SMA 50 Length: 221.32\n",
      "SMA 50 Reward: 2.32\n",
      "SMA 250 pgloss: -0.0016255832583810842\n",
      "SMA 250 vloss: 0.1473437362164259\n",
      "SMA 250 intrinsic: 0.0035405603158287706\n",
      "SMA 250 loss: 0.047210196919739246\n",
      "SMA 250 entropy: 1.2418044242858888\n",
      "FPS: 346\n",
      "ETA: 0:34:49\n",
      "Max reward: 14.0\n",
      "Number of games: 1462\n",
      "SOFTMAX:  tensor([0.1249, 0.3389, 0.2550, 0.2811], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 275 / 976 ----------\n",
      "SMA 50 Length: 200.7\n",
      "SMA 50 Reward: 2.02\n",
      "SMA 250 pgloss: -0.0016031496092728048\n",
      "SMA 250 vloss: 0.14637037044763565\n",
      "SMA 250 intrinsic: 0.0035210060845129194\n",
      "SMA 250 loss: 0.0467494984716177\n",
      "SMA 250 entropy: 1.241626883983612\n",
      "FPS: 346\n",
      "ETA: 0:34:33\n",
      "Max reward: 16.0\n",
      "Number of games: 1488\n",
      "SOFTMAX:  tensor([0.0367, 0.3939, 0.4700, 0.0994], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 280 / 976 ----------\n",
      "SMA 50 Length: 198.68\n",
      "SMA 50 Reward: 1.94\n",
      "SMA 250 pgloss: -0.0015817880498761952\n",
      "SMA 250 vloss: 0.14529718905687333\n",
      "SMA 250 intrinsic: 0.0035131488903425634\n",
      "SMA 250 loss: 0.04624292968958616\n",
      "SMA 250 entropy: 1.241193865299225\n",
      "FPS: 346\n",
      "ETA: 0:34:19\n",
      "Max reward: 16.0\n",
      "Number of games: 1517\n",
      "SOFTMAX:  tensor([0.1043, 0.3459, 0.3103, 0.2395], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 285 / 976 ----------\n",
      "SMA 50 Length: 202.8\n",
      "SMA 50 Reward: 1.98\n",
      "SMA 250 pgloss: -0.0015720718859047339\n",
      "SMA 250 vloss: 0.1452878740131855\n",
      "SMA 250 intrinsic: 0.0035024789036251606\n",
      "SMA 250 loss: 0.04627038136869669\n",
      "SMA 250 entropy: 1.2400742120742798\n",
      "FPS: 346\n",
      "ETA: 0:34:04\n",
      "Max reward: 16.0\n",
      "Number of games: 1544\n",
      "SOFTMAX:  tensor([0.1270, 0.3459, 0.2429, 0.2842], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 290 / 976 ----------\n",
      "SMA 50 Length: 220.1\n",
      "SMA 50 Reward: 2.56\n",
      "SMA 250 pgloss: -0.0016029825281984812\n",
      "SMA 250 vloss: 0.1438600340783596\n",
      "SMA 250 intrinsic: 0.0034840472396463155\n",
      "SMA 250 loss: 0.04554832710325718\n",
      "SMA 250 entropy: 1.2389353942871093\n",
      "FPS: 346\n",
      "ETA: 0:33:49\n",
      "Max reward: 16.0\n",
      "Number of games: 1568\n",
      "SOFTMAX:  tensor([0.0936, 0.3581, 0.3072, 0.2411], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 295 / 976 ----------\n",
      "SMA 50 Length: 224.0\n",
      "SMA 50 Reward: 2.5\n",
      "SMA 250 pgloss: -0.001612723262671352\n",
      "SMA 250 vloss: 0.1430557597577572\n",
      "SMA 250 intrinsic: 0.0034683077204972507\n",
      "SMA 250 loss: 0.04515775239467621\n",
      "SMA 250 entropy: 1.2378702335357665\n",
      "FPS: 346\n",
      "ETA: 0:33:33\n",
      "Max reward: 16.0\n",
      "Number of games: 1590\n",
      "SOFTMAX:  tensor([0.0747, 0.3463, 0.3384, 0.2406], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 300 / 976 ----------\n",
      "SMA 50 Length: 219.72\n",
      "SMA 50 Reward: 2.26\n",
      "SMA 250 pgloss: -0.0015789363471958495\n",
      "SMA 250 vloss: 0.14316337868571283\n",
      "SMA 250 intrinsic: 0.0034423696417361497\n",
      "SMA 250 loss: 0.04526588777452707\n",
      "SMA 250 entropy: 1.236843285560608\n",
      "FPS: 346\n",
      "ETA: 0:33:18\n",
      "Max reward: 16.0\n",
      "Number of games: 1615\n",
      "SOFTMAX:  tensor([0.0663, 0.3324, 0.4038, 0.1975], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 305 / 976 ----------\n",
      "SMA 50 Length: 223.02\n",
      "SMA 50 Reward: 2.4\n",
      "SMA 250 pgloss: -0.0015920763649446598\n",
      "SMA 250 vloss: 0.14423492397367954\n",
      "SMA 250 intrinsic: 0.0034268495608121155\n",
      "SMA 250 loss: 0.04580308583378792\n",
      "SMA 250 entropy: 1.2361150121688842\n",
      "FPS: 346\n",
      "ETA: 0:33:03\n",
      "Max reward: 16.0\n",
      "Number of games: 1640\n",
      "SOFTMAX:  tensor([0.1580, 0.2749, 0.2127, 0.3545], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 310 / 976 ----------\n",
      "SMA 50 Length: 221.88\n",
      "SMA 50 Reward: 2.44\n",
      "SMA 250 pgloss: -0.0015117806308371656\n",
      "SMA 250 vloss: 0.14354578875005244\n",
      "SMA 250 intrinsic: 0.0034185209041461347\n",
      "SMA 250 loss: 0.04554640444368124\n",
      "SMA 250 entropy: 1.2357354874610902\n",
      "FPS: 346\n",
      "ETA: 0:32:48\n",
      "Max reward: 16.0\n",
      "Number of games: 1667\n",
      "SOFTMAX:  tensor([0.1719, 0.2819, 0.2476, 0.2986], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 315 / 976 ----------\n",
      "SMA 50 Length: 215.36\n",
      "SMA 50 Reward: 2.28\n",
      "SMA 250 pgloss: -0.0015334033384970097\n",
      "SMA 250 vloss: 0.14326735474169255\n",
      "SMA 250 intrinsic: 0.003407658477779478\n",
      "SMA 250 loss: 0.045400212071836\n",
      "SMA 250 entropy: 1.2350031242370605\n",
      "FPS: 346\n",
      "ETA: 0:32:33\n",
      "Max reward: 16.0\n",
      "Number of games: 1692\n",
      "SOFTMAX:  tensor([0.1930, 0.2644, 0.2136, 0.3289], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 320 / 976 ----------\n",
      "SMA 50 Length: 199.88\n",
      "SMA 50 Reward: 1.9\n",
      "SMA 250 pgloss: -0.001442026874381554\n",
      "SMA 250 vloss: 0.1434324335604906\n",
      "SMA 250 intrinsic: 0.003404149533715099\n",
      "SMA 250 loss: 0.04555141262710095\n",
      "SMA 250 entropy: 1.236138889312744\n",
      "FPS: 346\n",
      "ETA: 0:32:19\n",
      "Max reward: 16.0\n",
      "Number of games: 1722\n",
      "SOFTMAX:  tensor([0.1881, 0.2497, 0.1993, 0.3630], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 325 / 976 ----------\n",
      "SMA 50 Length: 194.34\n",
      "SMA 50 Reward: 1.82\n",
      "SMA 250 pgloss: -0.001283528272393596\n",
      "SMA 250 vloss: 0.1442015168815851\n",
      "SMA 250 intrinsic: 0.003415367293637246\n",
      "SMA 250 loss: 0.04606100390106439\n",
      "SMA 250 entropy: 1.237811339855194\n",
      "FPS: 346\n",
      "ETA: 0:32:04\n",
      "Max reward: 16.0\n",
      "Number of games: 1749\n",
      "SOFTMAX:  tensor([0.1392, 0.2493, 0.2059, 0.4056], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 330 / 976 ----------\n",
      "SMA 50 Length: 181.08\n",
      "SMA 50 Reward: 1.46\n",
      "SMA 250 pgloss: -0.0012800612079263375\n",
      "SMA 250 vloss: 0.14280396755039693\n",
      "SMA 250 intrinsic: 0.0034238109621219336\n",
      "SMA 250 loss: 0.04535889577120542\n",
      "SMA 250 entropy: 1.238151366710663\n",
      "FPS: 346\n",
      "ETA: 0:31:50\n",
      "Max reward: 16.0\n",
      "Number of games: 1782\n",
      "SOFTMAX:  tensor([6.1635e-04, 9.8278e-01, 1.1646e-02, 4.9547e-03], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 335 / 976 ----------\n",
      "SMA 50 Length: 171.16\n",
      "SMA 50 Reward: 1.18\n",
      "SMA 250 pgloss: -0.001182014625734155\n",
      "SMA 250 vloss: 0.14326298271119595\n",
      "SMA 250 intrinsic: 0.0034318122235126795\n",
      "SMA 250 loss: 0.04569022949784994\n",
      "SMA 250 entropy: 1.237962387084961\n",
      "FPS: 346\n",
      "ETA: 0:31:35\n",
      "Max reward: 16.0\n",
      "Number of games: 1813\n",
      "SOFTMAX:  tensor([0.0747, 0.3385, 0.3951, 0.1916], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 340 / 976 ----------\n",
      "SMA 50 Length: 192.2\n",
      "SMA 50 Reward: 1.64\n",
      "SMA 250 pgloss: -0.00117340954327301\n",
      "SMA 250 vloss: 0.14186638723313807\n",
      "SMA 250 intrinsic: 0.00344516077497974\n",
      "SMA 250 loss: 0.04499978622049093\n",
      "SMA 250 entropy: 1.2379999141693114\n",
      "FPS: 346\n",
      "ETA: 0:31:21\n",
      "Max reward: 16.0\n",
      "Number of games: 1841\n",
      "SOFTMAX:  tensor([0.0547, 0.3390, 0.4625, 0.1438], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 345 / 976 ----------\n",
      "SMA 50 Length: 204.4\n",
      "SMA 50 Reward: 1.92\n",
      "SMA 250 pgloss: -0.0011057673374125442\n",
      "SMA 250 vloss: 0.14114076188206673\n",
      "SMA 250 intrinsic: 0.003449669346679002\n",
      "SMA 250 loss: 0.04468269395828247\n",
      "SMA 250 entropy: 1.2390960006713867\n",
      "FPS: 346\n",
      "ETA: 0:31:06\n",
      "Max reward: 16.0\n",
      "Number of games: 1867\n",
      "SOFTMAX:  tensor([0.1581, 0.2997, 0.2238, 0.3184], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 350 / 976 ----------\n",
      "SMA 50 Length: 211.34\n",
      "SMA 50 Reward: 2.04\n",
      "SMA 250 pgloss: -0.0009177554306625098\n",
      "SMA 250 vloss: 0.14128472186625005\n",
      "SMA 250 intrinsic: 0.003445879364851862\n",
      "SMA 250 loss: 0.0449537286683917\n",
      "SMA 250 entropy: 1.238543857574463\n",
      "FPS: 346\n",
      "ETA: 0:30:51\n",
      "Max reward: 16.0\n",
      "Number of games: 1891\n",
      "SOFTMAX:  tensor([0.0925, 0.4217, 0.2527, 0.2331], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 355 / 976 ----------\n",
      "SMA 50 Length: 219.02\n",
      "SMA 50 Reward: 2.26\n",
      "SMA 250 pgloss: -0.0008761959472722082\n",
      "SMA 250 vloss: 0.1435556320846081\n",
      "SMA 250 intrinsic: 0.003430341285187751\n",
      "SMA 250 loss: 0.04613803635537624\n",
      "SMA 250 entropy: 1.2381792011260986\n",
      "FPS: 346\n",
      "ETA: 0:30:35\n",
      "Max reward: 16.0\n",
      "Number of games: 1916\n",
      "SOFTMAX:  tensor([0.1145, 0.3644, 0.2853, 0.2358], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 360 / 976 ----------\n",
      "SMA 50 Length: 208.12\n",
      "SMA 50 Reward: 2.04\n",
      "SMA 250 pgloss: -0.0008887337780142843\n",
      "SMA 250 vloss: 0.14234456992149352\n",
      "SMA 250 intrinsic: 0.0034397258241660894\n",
      "SMA 250 loss: 0.0454980787858367\n",
      "SMA 250 entropy: 1.2392736349105835\n",
      "FPS: 346\n",
      "ETA: 0:30:21\n",
      "Max reward: 16.0\n",
      "Number of games: 1942\n",
      "SOFTMAX:  tensor([0.1174, 0.3448, 0.3074, 0.2303], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 365 / 976 ----------\n",
      "SMA 50 Length: 208.36\n",
      "SMA 50 Reward: 1.96\n",
      "SMA 250 pgloss: -0.0009183721913541376\n",
      "SMA 250 vloss: 0.141515023291111\n",
      "SMA 250 intrinsic: 0.003417780836112797\n",
      "SMA 250 loss: 0.04505560320615769\n",
      "SMA 250 entropy: 1.2391768264770509\n",
      "FPS: 346\n",
      "ETA: 0:30:06\n",
      "Max reward: 16.0\n",
      "Number of games: 1969\n",
      "SOFTMAX:  tensor([0.1134, 0.3313, 0.3182, 0.2371], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 370 / 976 ----------\n",
      "SMA 50 Length: 204.54\n",
      "SMA 50 Reward: 1.92\n",
      "SMA 250 pgloss: -0.0009391694528567314\n",
      "SMA 250 vloss: 0.1422552746683359\n",
      "SMA 250 intrinsic: 0.003386670372914523\n",
      "SMA 250 loss: 0.04542535244673491\n",
      "SMA 250 entropy: 1.238155785560608\n",
      "FPS: 346\n",
      "ETA: 0:29:51\n",
      "Max reward: 16.0\n",
      "Number of games: 1994\n",
      "SOFTMAX:  tensor([0.0632, 0.2697, 0.4333, 0.2338], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 375 / 976 ----------\n",
      "SMA 50 Length: 207.84\n",
      "SMA 50 Reward: 2.06\n",
      "SMA 250 pgloss: -0.0008823219404839619\n",
      "SMA 250 vloss: 0.1419300739169121\n",
      "SMA 250 intrinsic: 0.0033724048822186886\n",
      "SMA 250 loss: 0.04533739272505045\n",
      "SMA 250 entropy: 1.2372661290168763\n",
      "FPS: 346\n",
      "ETA: 0:29:35\n",
      "Max reward: 16.0\n",
      "Number of games: 2020\n",
      "SOFTMAX:  tensor([0.1225, 0.2491, 0.3589, 0.2695], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 380 / 976 ----------\n",
      "SMA 50 Length: 211.2\n",
      "SMA 50 Reward: 2.18\n",
      "SMA 250 pgloss: -0.0009793408671212092\n",
      "SMA 250 vloss: 0.14105808505415918\n",
      "SMA 250 intrinsic: 0.0033560720984824\n",
      "SMA 250 loss: 0.04479915522038937\n",
      "SMA 250 entropy: 1.2375273370742799\n",
      "FPS: 346\n",
      "ETA: 0:29:20\n",
      "Max reward: 16.0\n",
      "Number of games: 2048\n",
      "SOFTMAX:  tensor([0.1026, 0.3099, 0.4269, 0.1606], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 385 / 976 ----------\n",
      "SMA 50 Length: 222.2\n",
      "SMA 50 Reward: 2.36\n",
      "SMA 250 pgloss: -0.0009839836889696015\n",
      "SMA 250 vloss: 0.14089454731345177\n",
      "SMA 250 intrinsic: 0.003346358819399029\n",
      "SMA 250 loss: 0.044703364804387094\n",
      "SMA 250 entropy: 1.2379962759017944\n",
      "FPS: 346\n",
      "ETA: 0:29:05\n",
      "Max reward: 16.0\n",
      "Number of games: 2069\n",
      "SOFTMAX:  tensor([0.1216, 0.3195, 0.3938, 0.1650], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 390 / 976 ----------\n",
      "SMA 50 Length: 226.52\n",
      "SMA 50 Reward: 2.44\n",
      "SMA 250 pgloss: -0.0009887552137188321\n",
      "SMA 250 vloss: 0.1407864590585232\n",
      "SMA 250 intrinsic: 0.0033376370584592223\n",
      "SMA 250 loss: 0.04463027560710907\n",
      "SMA 250 entropy: 1.2387099494934082\n",
      "FPS: 346\n",
      "ETA: 0:28:50\n",
      "Max reward: 16.0\n",
      "Number of games: 2090\n",
      "SOFTMAX:  tensor([0.1850, 0.2521, 0.2264, 0.3365], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 395 / 976 ----------\n",
      "SMA 50 Length: 240.3\n",
      "SMA 50 Reward: 2.86\n",
      "SMA 250 pgloss: -0.001040638278282131\n",
      "SMA 250 vloss: 0.14035569271445275\n",
      "SMA 250 intrinsic: 0.003320817781612277\n",
      "SMA 250 loss: 0.04434486006200314\n",
      "SMA 250 entropy: 1.2396174139976501\n",
      "FPS: 347\n",
      "ETA: 0:28:35\n",
      "Max reward: 16.0\n",
      "Number of games: 2117\n",
      "SOFTMAX:  tensor([0.1874, 0.2680, 0.2423, 0.3023], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 400 / 976 ----------\n",
      "SMA 50 Length: 212.74\n",
      "SMA 50 Reward: 2.24\n",
      "SMA 250 pgloss: -0.0010470555057254386\n",
      "SMA 250 vloss: 0.14143575260043145\n",
      "SMA 250 intrinsic: 0.0033174140099436044\n",
      "SMA 250 loss: 0.04485817911475897\n",
      "SMA 250 entropy: 1.240632098197937\n",
      "FPS: 347\n",
      "ETA: 0:28:21\n",
      "Max reward: 16.0\n",
      "Number of games: 2144\n",
      "SOFTMAX:  tensor([0.2114, 0.2448, 0.2316, 0.3121], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 405 / 976 ----------\n",
      "SMA 50 Length: 201.68\n",
      "SMA 50 Reward: 1.88\n",
      "SMA 250 pgloss: -0.0009787000223150245\n",
      "SMA 250 vloss: 0.13961863508820535\n",
      "SMA 250 intrinsic: 0.003314835265278816\n",
      "SMA 250 loss: 0.044009832106530665\n",
      "SMA 250 entropy: 1.2410392870903015\n",
      "FPS: 346\n",
      "ETA: 0:28:07\n",
      "Max reward: 16.0\n",
      "Number of games: 2169\n",
      "SOFTMAX:  tensor([0.2021, 0.2661, 0.2380, 0.2938], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 410 / 976 ----------\n",
      "SMA 50 Length: 204.32\n",
      "SMA 50 Reward: 1.9\n",
      "SMA 250 pgloss: -0.0009507848539942643\n",
      "SMA 250 vloss: 0.14022173748910427\n",
      "SMA 250 intrinsic: 0.0032890073359012604\n",
      "SMA 250 loss: 0.04434485396742821\n",
      "SMA 250 entropy: 1.2407615122795106\n",
      "FPS: 346\n",
      "ETA: 0:27:53\n",
      "Max reward: 16.0\n",
      "Number of games: 2197\n",
      "SOFTMAX:  tensor([0.2061, 0.2592, 0.2674, 0.2673], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 415 / 976 ----------\n",
      "SMA 50 Length: 214.72\n",
      "SMA 50 Reward: 2.3\n",
      "SMA 250 pgloss: -0.0010978623118571705\n",
      "SMA 250 vloss: 0.1400666239708662\n",
      "SMA 250 intrinsic: 0.003266159306745976\n",
      "SMA 250 loss: 0.044114022187888624\n",
      "SMA 250 entropy: 1.2410713930130004\n",
      "FPS: 346\n",
      "ETA: 0:27:39\n",
      "Max reward: 16.0\n",
      "Number of games: 2222\n",
      "SOFTMAX:  tensor([0.1912, 0.2312, 0.4454, 0.1322], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 420 / 976 ----------\n",
      "SMA 50 Length: 211.36\n",
      "SMA 50 Reward: 2.22\n",
      "SMA 250 pgloss: -0.001087776412940002\n",
      "SMA 250 vloss: 0.13883184318244457\n",
      "SMA 250 intrinsic: 0.0032577410941012205\n",
      "SMA 250 loss: 0.04348270550370217\n",
      "SMA 250 entropy: 1.2422720036506654\n",
      "FPS: 346\n",
      "ETA: 0:27:25\n",
      "Max reward: 16.0\n",
      "Number of games: 2250\n",
      "SOFTMAX:  tensor([0.2990, 0.2559, 0.2031, 0.2420], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 425 / 976 ----------\n",
      "SMA 50 Length: 215.3\n",
      "SMA 50 Reward: 2.18\n",
      "SMA 250 pgloss: -0.0010346047195344\n",
      "SMA 250 vloss: 0.13914626954495907\n",
      "SMA 250 intrinsic: 0.003237398324068636\n",
      "SMA 250 loss: 0.04365837605297566\n",
      "SMA 250 entropy: 1.2440077214241028\n",
      "FPS: 346\n",
      "ETA: 0:27:11\n",
      "Max reward: 16.0\n",
      "Number of games: 2274\n",
      "SOFTMAX:  tensor([0.2476, 0.2462, 0.2410, 0.2651], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 430 / 976 ----------\n",
      "SMA 50 Length: 212.28\n",
      "SMA 50 Reward: 2.1\n",
      "SMA 250 pgloss: -0.0008632634769746801\n",
      "SMA 250 vloss: 0.13839315635710955\n",
      "SMA 250 intrinsic: 0.003228511858265847\n",
      "SMA 250 loss: 0.04340000832080841\n",
      "SMA 250 entropy: 1.2466653409004211\n",
      "FPS: 345\n",
      "ETA: 0:26:57\n",
      "Max reward: 16.0\n",
      "Number of games: 2302\n",
      "SOFTMAX:  tensor([0.2283, 0.2612, 0.2350, 0.2755], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 435 / 976 ----------\n",
      "SMA 50 Length: 199.24\n",
      "SMA 50 Reward: 1.72\n",
      "SMA 250 pgloss: -0.0009406568355479977\n",
      "SMA 250 vloss: 0.13821308251470327\n",
      "SMA 250 intrinsic: 0.003220199177507311\n",
      "SMA 250 loss: 0.04318914430588484\n",
      "SMA 250 entropy: 1.248837026119232\n",
      "FPS: 345\n",
      "ETA: 0:26:43\n",
      "Max reward: 16.0\n",
      "Number of games: 2327\n",
      "SOFTMAX:  tensor([0.2365, 0.2423, 0.2315, 0.2897], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 440 / 976 ----------\n",
      "SMA 50 Length: 203.88\n",
      "SMA 50 Reward: 2.0\n",
      "SMA 250 pgloss: -0.0010004559417575365\n",
      "SMA 250 vloss: 0.13796332272142173\n",
      "SMA 250 intrinsic: 0.003215413014870137\n",
      "SMA 250 loss: 0.0429621314406395\n",
      "SMA 250 entropy: 1.2509537167549134\n",
      "FPS: 345\n",
      "ETA: 0:26:29\n",
      "Max reward: 16.0\n",
      "Number of games: 2352\n",
      "SOFTMAX:  tensor([0.2707, 0.2286, 0.2326, 0.2681], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 445 / 976 ----------\n",
      "SMA 50 Length: 239.26\n",
      "SMA 50 Reward: 2.94\n",
      "SMA 250 pgloss: -0.0009747721806488698\n",
      "SMA 250 vloss: 0.13850076318532228\n",
      "SMA 250 intrinsic: 0.003201285325456411\n",
      "SMA 250 loss: 0.0431984162479639\n",
      "SMA 250 entropy: 1.253859676837921\n",
      "FPS: 345\n",
      "ETA: 0:26:15\n",
      "Max reward: 16.0\n",
      "Number of games: 2373\n",
      "SOFTMAX:  tensor([0.1683, 0.2867, 0.3745, 0.1705], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 450 / 976 ----------\n",
      "SMA 50 Length: 239.52\n",
      "SMA 50 Reward: 2.82\n",
      "SMA 250 pgloss: -0.0009556391994556179\n",
      "SMA 250 vloss: 0.13647922284156083\n",
      "SMA 250 intrinsic: 0.00321389884268865\n",
      "SMA 250 loss: 0.042151617765426634\n",
      "SMA 250 entropy: 1.2566177411079407\n",
      "FPS: 345\n",
      "ETA: 0:26:01\n",
      "Max reward: 16.0\n",
      "Number of games: 2398\n",
      "SOFTMAX:  tensor([0.1529, 0.3220, 0.2742, 0.2509], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 455 / 976 ----------\n",
      "SMA 50 Length: 209.58\n",
      "SMA 50 Reward: 2.16\n",
      "SMA 250 pgloss: -0.0009328670409886399\n",
      "SMA 250 vloss: 0.13632450012117625\n",
      "SMA 250 intrinsic: 0.0032166479961015285\n",
      "SMA 250 loss: 0.042038078851997854\n",
      "SMA 250 entropy: 1.259565227508545\n",
      "FPS: 345\n",
      "ETA: 0:25:46\n",
      "Max reward: 16.0\n",
      "Number of games: 2425\n",
      "SOFTMAX:  tensor([0.1187, 0.3125, 0.4515, 0.1173], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 460 / 976 ----------\n",
      "SMA 50 Length: 203.18\n",
      "SMA 50 Reward: 2.0\n",
      "SMA 250 pgloss: -0.0009094934178137919\n",
      "SMA 250 vloss: 0.13645801417529582\n",
      "SMA 250 intrinsic: 0.003217916963156313\n",
      "SMA 250 loss: 0.0420564895644784\n",
      "SMA 250 entropy: 1.2631512250900268\n",
      "FPS: 345\n",
      "ETA: 0:25:32\n",
      "Max reward: 16.0\n",
      "Number of games: 2450\n",
      "SOFTMAX:  tensor([0.2099, 0.2741, 0.2532, 0.2628], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 465 / 976 ----------\n",
      "SMA 50 Length: 223.56\n",
      "SMA 50 Reward: 2.4\n",
      "SMA 250 pgloss: -0.0009335145428631222\n",
      "SMA 250 vloss: 0.13692013293504715\n",
      "SMA 250 intrinsic: 0.0032033255877904595\n",
      "SMA 250 loss: 0.04220387811213732\n",
      "SMA 250 entropy: 1.2661337127685548\n",
      "FPS: 345\n",
      "ETA: 0:25:17\n",
      "Max reward: 16.0\n",
      "Number of games: 2474\n",
      "SOFTMAX:  tensor([0.2110, 0.2699, 0.3513, 0.1678], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 470 / 976 ----------\n",
      "SMA 50 Length: 231.54\n",
      "SMA 50 Reward: 2.64\n",
      "SMA 250 pgloss: -0.0008812924919038779\n",
      "SMA 250 vloss: 0.13723792266845702\n",
      "SMA 250 intrinsic: 0.0032105786292813718\n",
      "SMA 250 loss: 0.04233921609073878\n",
      "SMA 250 entropy: 1.2699226584434509\n",
      "FPS: 345\n",
      "ETA: 0:25:02\n",
      "Max reward: 16.0\n",
      "Number of games: 2497\n",
      "SOFTMAX:  tensor([0.2277, 0.2462, 0.2382, 0.2879], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 475 / 976 ----------\n",
      "SMA 50 Length: 236.82\n",
      "SMA 50 Reward: 2.82\n",
      "SMA 250 pgloss: -0.0010022387325152521\n",
      "SMA 250 vloss: 0.13764534491300584\n",
      "SMA 250 intrinsic: 0.00320805355720222\n",
      "SMA 250 loss: 0.04233383535593748\n",
      "SMA 250 entropy: 1.2743299403190613\n",
      "FPS: 345\n",
      "ETA: 0:24:47\n",
      "Max reward: 16.0\n",
      "Number of games: 2520\n",
      "SOFTMAX:  tensor([0.2741, 0.2762, 0.1659, 0.2839], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 480 / 976 ----------\n",
      "SMA 50 Length: 217.1\n",
      "SMA 50 Reward: 2.28\n",
      "SMA 250 pgloss: -0.0010071034172869987\n",
      "SMA 250 vloss: 0.13797473740577698\n",
      "SMA 250 intrinsic: 0.0032077469434589148\n",
      "SMA 250 loss: 0.04242036860436201\n",
      "SMA 250 entropy: 1.2779948573112487\n",
      "FPS: 345\n",
      "ETA: 0:24:32\n",
      "Max reward: 16.0\n",
      "Number of games: 2549\n",
      "SOFTMAX:  tensor([0.2717, 0.2181, 0.2547, 0.2554], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 485 / 976 ----------\n",
      "SMA 50 Length: 198.94\n",
      "SMA 50 Reward: 1.84\n",
      "SMA 250 pgloss: -0.0008993082280503586\n",
      "SMA 250 vloss: 0.1387032785564661\n",
      "SMA 250 intrinsic: 0.0032111302614212037\n",
      "SMA 250 loss: 0.042819751359522346\n",
      "SMA 250 entropy: 1.2816290063858031\n",
      "FPS: 345\n",
      "ETA: 0:24:17\n",
      "Max reward: 16.0\n",
      "Number of games: 2576\n",
      "SOFTMAX:  tensor([0.2745, 0.2563, 0.1943, 0.2749], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 490 / 976 ----------\n",
      "SMA 50 Length: 203.64\n",
      "SMA 50 Reward: 1.92\n",
      "SMA 250 pgloss: -0.0008328658576356248\n",
      "SMA 250 vloss: 0.13884458823502063\n",
      "SMA 250 intrinsic: 0.003196498983539641\n",
      "SMA 250 loss: 0.042886484287679195\n",
      "SMA 250 entropy: 1.2851472215652466\n",
      "FPS: 345\n",
      "ETA: 0:24:02\n",
      "Max reward: 16.0\n",
      "Number of games: 2604\n",
      "SOFTMAX:  tensor([0.2761, 0.2343, 0.1864, 0.3031], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 495 / 976 ----------\n",
      "SMA 50 Length: 206.52\n",
      "SMA 50 Reward: 2.06\n",
      "SMA 250 pgloss: -0.0009185594139853493\n",
      "SMA 250 vloss: 0.14138665963709354\n",
      "SMA 250 intrinsic: 0.0031831113277003167\n",
      "SMA 250 loss: 0.04401297317445278\n",
      "SMA 250 entropy: 1.2880898866653443\n",
      "FPS: 345\n",
      "ETA: 0:23:47\n",
      "Max reward: 16.0\n",
      "Number of games: 2628\n",
      "SOFTMAX:  tensor([0.2898, 0.2091, 0.2444, 0.2566], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 500 / 976 ----------\n",
      "SMA 50 Length: 209.86\n",
      "SMA 50 Reward: 2.36\n",
      "SMA 250 pgloss: -0.0010090950444573536\n",
      "SMA 250 vloss: 0.1422452376037836\n",
      "SMA 250 intrinsic: 0.0031664979672059415\n",
      "SMA 250 loss: 0.04430519773811102\n",
      "SMA 250 entropy: 1.2904163246154785\n",
      "FPS: 345\n",
      "ETA: 0:23:32\n",
      "Max reward: 16.0\n",
      "Number of games: 2655\n",
      "SOFTMAX:  tensor([0.2948, 0.2177, 0.1544, 0.3331], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 505 / 976 ----------\n",
      "SMA 50 Length: 211.7\n",
      "SMA 50 Reward: 2.38\n",
      "SMA 250 pgloss: -0.0009455803723685676\n",
      "SMA 250 vloss: 0.1435831787735224\n",
      "SMA 250 intrinsic: 0.0031559190033003687\n",
      "SMA 250 loss: 0.04498947957903147\n",
      "SMA 250 entropy: 1.292826497077942\n",
      "FPS: 345\n",
      "ETA: 0:23:17\n",
      "Max reward: 16.0\n",
      "Number of games: 2678\n",
      "SOFTMAX:  tensor([0.2172, 0.2820, 0.2644, 0.2363], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 510 / 976 ----------\n",
      "SMA 50 Length: 230.82\n",
      "SMA 50 Reward: 2.72\n",
      "SMA 250 pgloss: -0.0009722656620724592\n",
      "SMA 250 vloss: 0.1449975755214691\n",
      "SMA 250 intrinsic: 0.0031475482303649186\n",
      "SMA 250 loss: 0.045617652505636215\n",
      "SMA 250 entropy: 1.2954435033798217\n",
      "FPS: 345\n",
      "ETA: 0:23:02\n",
      "Max reward: 16.0\n",
      "Number of games: 2702\n",
      "SOFTMAX:  tensor([0.2518, 0.2191, 0.1886, 0.3405], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 515 / 976 ----------\n",
      "SMA 50 Length: 249.34\n",
      "SMA 50 Reward: 3.08\n",
      "SMA 250 pgloss: -0.0009046951827622251\n",
      "SMA 250 vloss: 0.14615267530083656\n",
      "SMA 250 intrinsic: 0.003147287817671895\n",
      "SMA 250 loss: 0.046203707784414294\n",
      "SMA 250 entropy: 1.2983967566490173\n",
      "FPS: 345\n",
      "ETA: 0:22:47\n",
      "Max reward: 16.0\n",
      "Number of games: 2725\n",
      "SOFTMAX:  tensor([0.2538, 0.2207, 0.1941, 0.3314], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 520 / 976 ----------\n",
      "SMA 50 Length: 232.02\n",
      "SMA 50 Reward: 2.72\n",
      "SMA 250 pgloss: -0.0008327013153611915\n",
      "SMA 250 vloss: 0.14851871243119238\n",
      "SMA 250 intrinsic: 0.003128933460917324\n",
      "SMA 250 loss: 0.0474225602671504\n",
      "SMA 250 entropy: 1.3002047533988952\n",
      "FPS: 345\n",
      "ETA: 0:22:32\n",
      "Max reward: 16.0\n",
      "Number of games: 2748\n",
      "SOFTMAX:  tensor([0.2564, 0.2397, 0.2301, 0.2738], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 525 / 976 ----------\n",
      "SMA 50 Length: 232.58\n",
      "SMA 50 Reward: 2.8\n",
      "SMA 250 pgloss: -0.000860384177431115\n",
      "SMA 250 vloss: 0.1498883522003889\n",
      "SMA 250 intrinsic: 0.0031054007913917303\n",
      "SMA 250 loss: 0.048051285713911054\n",
      "SMA 250 entropy: 1.3016253328323364\n",
      "FPS: 345\n",
      "ETA: 0:22:17\n",
      "Max reward: 16.0\n",
      "Number of games: 2769\n",
      "SOFTMAX:  tensor([0.1991, 0.2968, 0.3721, 0.1320], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 530 / 976 ----------\n",
      "SMA 50 Length: 243.94\n",
      "SMA 50 Reward: 3.04\n",
      "SMA 250 pgloss: -0.0009413943712715991\n",
      "SMA 250 vloss: 0.15210726761817933\n",
      "SMA 250 intrinsic: 0.003082234184257686\n",
      "SMA 250 loss: 0.04905773661285639\n",
      "SMA 250 entropy: 1.3027251634597778\n",
      "FPS: 345\n",
      "ETA: 0:22:03\n",
      "Max reward: 16.0\n",
      "Number of games: 2793\n",
      "SOFTMAX:  tensor([0.2590, 0.1963, 0.1884, 0.3563], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 535 / 976 ----------\n",
      "SMA 50 Length: 243.36\n",
      "SMA 50 Reward: 2.96\n",
      "SMA 250 pgloss: -0.0009292334945057519\n",
      "SMA 250 vloss: 0.1509925419986248\n",
      "SMA 250 intrinsic: 0.0030698208482936025\n",
      "SMA 250 loss: 0.04848678193986416\n",
      "SMA 250 entropy: 1.3040127992630004\n",
      "FPS: 345\n",
      "ETA: 0:21:48\n",
      "Max reward: 16.0\n",
      "Number of games: 2815\n",
      "SOFTMAX:  tensor([0.1834, 0.2988, 0.3579, 0.1600], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 540 / 976 ----------\n",
      "SMA 50 Length: 228.52\n",
      "SMA 50 Reward: 2.74\n",
      "SMA 250 pgloss: -0.0008587913882220164\n",
      "SMA 250 vloss: 0.15231281653046608\n",
      "SMA 250 intrinsic: 0.0030725853415206074\n",
      "SMA 250 loss: 0.04918869073688984\n",
      "SMA 250 entropy: 1.3054463276863097\n",
      "FPS: 345\n",
      "ETA: 0:21:34\n",
      "Max reward: 16.0\n",
      "Number of games: 2839\n",
      "SOFTMAX:  tensor([0.2417, 0.1971, 0.1827, 0.3784], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 545 / 976 ----------\n",
      "SMA 50 Length: 210.36\n",
      "SMA 50 Reward: 2.26\n",
      "SMA 250 pgloss: -0.0008705175643844995\n",
      "SMA 250 vloss: 0.15130980417132378\n",
      "SMA 250 intrinsic: 0.003080509055405855\n",
      "SMA 250 loss: 0.04865290112793445\n",
      "SMA 250 entropy: 1.3065741896629333\n",
      "FPS: 345\n",
      "ETA: 0:21:19\n",
      "Max reward: 16.0\n",
      "Number of games: 2867\n",
      "SOFTMAX:  tensor([0.1846, 0.2772, 0.2704, 0.2678], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 550 / 976 ----------\n",
      "SMA 50 Length: 201.72\n",
      "SMA 50 Reward: 1.96\n",
      "SMA 250 pgloss: -0.0008478686496091541\n",
      "SMA 250 vloss: 0.15122139874100685\n",
      "SMA 250 intrinsic: 0.0030798855414614083\n",
      "SMA 250 loss: 0.04860514622926712\n",
      "SMA 250 entropy: 1.307884243965149\n",
      "FPS: 345\n",
      "ETA: 0:21:04\n",
      "Max reward: 16.0\n",
      "Number of games: 2894\n",
      "SOFTMAX:  tensor([0.2355, 0.2014, 0.1815, 0.3816], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 555 / 976 ----------\n",
      "SMA 50 Length: 224.02\n",
      "SMA 50 Reward: 2.5\n",
      "SMA 250 pgloss: -0.0008470803104282822\n",
      "SMA 250 vloss: 0.15067862994968892\n",
      "SMA 250 intrinsic: 0.0030663132639601825\n",
      "SMA 250 loss: 0.048319005742669105\n",
      "SMA 250 entropy: 1.3086614670753478\n",
      "FPS: 345\n",
      "ETA: 0:20:49\n",
      "Max reward: 16.0\n",
      "Number of games: 2916\n",
      "SOFTMAX:  tensor([0.2062, 0.3055, 0.3257, 0.1626], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 560 / 976 ----------\n",
      "SMA 50 Length: 232.46\n",
      "SMA 50 Reward: 2.72\n",
      "SMA 250 pgloss: -0.0007921794011199381\n",
      "SMA 250 vloss: 0.15191042090952397\n",
      "SMA 250 intrinsic: 0.003055687693879008\n",
      "SMA 250 loss: 0.04897173760086298\n",
      "SMA 250 entropy: 1.309564694404602\n",
      "FPS: 345\n",
      "ETA: 0:20:34\n",
      "Max reward: 16.0\n",
      "Number of games: 2940\n",
      "SOFTMAX:  tensor([0.2665, 0.2033, 0.1911, 0.3391], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 565 / 976 ----------\n",
      "SMA 50 Length: 198.36\n",
      "SMA 50 Reward: 1.9\n",
      "SMA 250 pgloss: -0.0007018945841991808\n",
      "SMA 250 vloss: 0.15232696561515333\n",
      "SMA 250 intrinsic: 0.0030504530598409476\n",
      "SMA 250 loss: 0.04925400310009718\n",
      "SMA 250 entropy: 1.310379277229309\n",
      "FPS: 345\n",
      "ETA: 0:20:20\n",
      "Max reward: 16.0\n",
      "Number of games: 2970\n",
      "SOFTMAX:  tensor([0.2691, 0.1921, 0.1703, 0.3685], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 570 / 976 ----------\n",
      "SMA 50 Length: 206.32\n",
      "SMA 50 Reward: 1.94\n",
      "SMA 250 pgloss: -0.0006991378628081293\n",
      "SMA 250 vloss: 0.15125424875319005\n",
      "SMA 250 intrinsic: 0.00304680854594335\n",
      "SMA 250 loss: 0.04870897872000933\n",
      "SMA 250 entropy: 1.3109504113197326\n",
      "FPS: 345\n",
      "ETA: 0:20:05\n",
      "Max reward: 16.0\n",
      "Number of games: 2996\n",
      "SOFTMAX:  tensor([0.1973, 0.2852, 0.4023, 0.1152], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 575 / 976 ----------\n",
      "SMA 50 Length: 203.94\n",
      "SMA 50 Reward: 1.96\n",
      "SMA 250 pgloss: -0.0006996687935097725\n",
      "SMA 250 vloss: 0.15042826728522776\n",
      "SMA 250 intrinsic: 0.0030338091044686735\n",
      "SMA 250 loss: 0.04828036015480757\n",
      "SMA 250 entropy: 1.3117052569389342\n",
      "FPS: 345\n",
      "ETA: 0:19:50\n",
      "Max reward: 16.0\n",
      "Number of games: 3025\n",
      "SOFTMAX:  tensor([0.2329, 0.2655, 0.3495, 0.1521], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 580 / 976 ----------\n",
      "SMA 50 Length: 196.78\n",
      "SMA 50 Reward: 1.86\n",
      "SMA 250 pgloss: -0.0008471345929792733\n",
      "SMA 250 vloss: 0.1505208951383829\n",
      "SMA 250 intrinsic: 0.003023306988645345\n",
      "SMA 250 loss: 0.048171701803803445\n",
      "SMA 250 entropy: 1.3120805802345277\n",
      "FPS: 345\n",
      "ETA: 0:19:35\n",
      "Max reward: 16.0\n",
      "Number of games: 3052\n",
      "SOFTMAX:  tensor([0.2728, 0.2154, 0.2267, 0.2851], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 585 / 976 ----------\n",
      "SMA 50 Length: 200.48\n",
      "SMA 50 Reward: 1.9\n",
      "SMA 250 pgloss: -0.0009939968875041813\n",
      "SMA 250 vloss: 0.1494963466078043\n",
      "SMA 250 intrinsic: 0.0030078113726340235\n",
      "SMA 250 loss: 0.047504283145070075\n",
      "SMA 250 entropy: 1.312494683742523\n",
      "FPS: 345\n",
      "ETA: 0:19:21\n",
      "Max reward: 16.0\n",
      "Number of games: 3082\n",
      "SOFTMAX:  tensor([0.2736, 0.1725, 0.1743, 0.3795], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 590 / 976 ----------\n",
      "SMA 50 Length: 207.04\n",
      "SMA 50 Reward: 2.02\n",
      "SMA 250 pgloss: -0.0010156810316184419\n",
      "SMA 250 vloss: 0.14964850737154484\n",
      "SMA 250 intrinsic: 0.0029886537082493307\n",
      "SMA 250 loss: 0.047539666831493375\n",
      "SMA 250 entropy: 1.3134453110694886\n",
      "FPS: 345\n",
      "ETA: 0:19:06\n",
      "Max reward: 16.0\n",
      "Number of games: 3104\n",
      "SOFTMAX:  tensor([0.2736, 0.2282, 0.2918, 0.2064], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 595 / 976 ----------\n",
      "SMA 50 Length: 214.88\n",
      "SMA 50 Reward: 2.18\n",
      "SMA 250 pgloss: -0.0010364418025055784\n",
      "SMA 250 vloss: 0.1504684098958969\n",
      "SMA 250 intrinsic: 0.002977288832888007\n",
      "SMA 250 loss: 0.047905123457312584\n",
      "SMA 250 entropy: 1.3146320037841797\n",
      "FPS: 345\n",
      "ETA: 0:18:51\n",
      "Max reward: 16.0\n",
      "Number of games: 3131\n",
      "SOFTMAX:  tensor([0.2417, 0.2367, 0.4109, 0.1107], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 600 / 976 ----------\n",
      "SMA 50 Length: 202.32\n",
      "SMA 50 Reward: 1.82\n",
      "SMA 250 pgloss: -0.0011055482634619693\n",
      "SMA 250 vloss: 0.15052561271190643\n",
      "SMA 250 intrinsic: 0.0029677493497729302\n",
      "SMA 250 loss: 0.04784265568107367\n",
      "SMA 250 entropy: 1.3157301421165466\n",
      "FPS: 345\n",
      "ETA: 0:18:36\n",
      "Max reward: 16.0\n",
      "Number of games: 3159\n",
      "SOFTMAX:  tensor([0.2901, 0.1607, 0.1816, 0.3676], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "---------- 605 / 976 ----------\n",
      "SMA 50 Length: 209.6\n",
      "SMA 50 Reward: 1.98\n",
      "SMA 250 pgloss: -0.00110658457356476\n",
      "SMA 250 vloss: 0.14867735496163367\n",
      "SMA 250 intrinsic: 0.002959323562681675\n",
      "SMA 250 loss: 0.04689814707636833\n",
      "SMA 250 entropy: 1.3166973147392274\n",
      "FPS: 345\n",
      "ETA: 0:18:21\n",
      "Max reward: 16.0\n",
      "Number of games: 3183\n",
      "SOFTMAX:  tensor([0.2808, 0.1544, 0.1734, 0.3914], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "for update in range(1, NUM_UPDATES):\n",
    "\n",
    "    obs, obs_d1, obs_d2, logps, actions, values, rewards, infos = next(rgen)\n",
    "\n",
    "    for _ in range(MB_EPOCHS):    # perform n updates on each minibatch\n",
    "        \n",
    "        for s in range(MB_SPLIT):  # split the batch into n chunks\n",
    "\n",
    "            indices = slice(s*SPLIT_LEN, s*SPLIT_LEN + SPLIT_LEN)\n",
    "            s_obs        = obs[indices]\n",
    "            s_obs_d1     = obs_d1[indices]\n",
    "            s_obs_d2     = obs_d2[indices]\n",
    "            s_logps_old  = logps[indices]\n",
    "            s_actions    = actions[indices]\n",
    "            s_rewards    = rewards[indices]\n",
    "            s_values_old = values[indices] \n",
    "\n",
    "            # update representation network S_{t+1}\n",
    "            pred_ae_1, z1 = ae_delta_1(s_obs, s_actions.cpu().numpy().astype(int))\n",
    "            citerion  = nn.MSELoss()\n",
    "            loss_ae_1 = citerion(pred_ae_1, s_obs_d1)\n",
    "            ae_optim_1.zero_grad()\n",
    "            loss_ae_1.backward()\n",
    "            nn.utils.clip_grad_norm_(ae_delta_1.parameters(), 0.5)\n",
    "            ae_optim_1.step()\n",
    "\n",
    "            # update representation network S_{t+2}\n",
    "            pred_ae_2, z2 = ae_delta_2(s_obs, s_actions.cpu().numpy().astype(int))\n",
    "            citerion  = nn.MSELoss()\n",
    "            loss_ae_2 = citerion(pred_ae_2, s_obs_d2)\n",
    "            ae_optim_2.zero_grad()\n",
    "            loss_ae_2.backward()\n",
    "            nn.utils.clip_grad_norm_(ae_delta_2.parameters(), 0.5)\n",
    "            ae_optim_2.step()\n",
    "\n",
    "            # compute intrinsic reward (inefficent TODO)\n",
    "            int1 = ((s_obs_d1 - pred_ae_1)**2).mean(dim=(1,2,3)) \n",
    "            int2 = ((s_obs_d2 - pred_ae_2)**2).mean(dim=(1,2,3)) \n",
    "            intrinsic_reward = int1.detach() + int2.detach()\n",
    "            # extrinsic + intrinsic, baselined for advantage\n",
    "            adv = s_rewards + 0.2*intrinsic_reward - s_values_old\n",
    "            adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "            # IMPORTANT this is being updated on OUTDATED z -- recompute? TODO TRY THIS FIRST IF NO WORK <<<<<<<<<<<<<<<<<<<<<\n",
    "            z1 = ae_delta_1.encode(s_obs).detach()\n",
    "            z2 = ae_delta_2.encode(s_obs).detach()\n",
    "            # ppo on actor critic\n",
    "            z_join    = T.cat((z1.detach(),z2.detach()), dim=1)\n",
    "            pi, v     = acc(z_join)\n",
    "            v         = v[:, 0]\n",
    "            dist      = Categorical(logits=pi)\n",
    "            s_entropy = dist.entropy().mean()\n",
    "            s_logps_new = dist.log_prob(s_actions)\n",
    "\n",
    "            # calculate surrogate loss (ratio == 1 on first iter, ln cancels)\n",
    "            ratio   = (s_logps_new - s_logps_old).exp() # e^x / e^x == e^(x-y)\n",
    "            L_CPI   = adv * ratio\n",
    "            L_CLAMP = adv * (ratio.clamp(1-CLIP, 1+CLIP))\n",
    "            L_CLIP  = -T.min(L_CPI, L_CLAMP).mean()\n",
    "            \n",
    "            # caculate value loss (try clamping as experiment)  # TODO : IMPORTANT TO SCALE INT HERE TOO?\n",
    "            V_L = ((v-(s_rewards + 0.2*intrinsic_reward))**2).mean()\n",
    "            \n",
    "            # final loss\n",
    "            loss = L_CLIP + 0.5 * V_L - 0.02 * s_entropy\n",
    "\n",
    "            acc_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(acc.parameters(), 0.5)\n",
    "            acc_optim.step()\n",
    "    \n",
    "    # decrease learning rates linearly\n",
    "    acc_scheduler.step(update)\n",
    "    ae_1_scheduler.step(update)\n",
    "    ae_2_scheduler.step(update)\n",
    "\n",
    "    # log stats\n",
    "    logger.update(\n",
    "        infos, L_CLIP.item(), V_L.item(), \n",
    "        s_entropy.item(), loss.item(), \n",
    "        intrinsic_reward.mean().item(), \n",
    "        print_rate=5\n",
    "    )\n",
    "    if update%5 == 0:\n",
    "        print('SOFTMAX: ', dist.probs[0])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "unet AutoEncoder diffs.ipynb",
   "provenance": [
    {
     "file_id": "1j75Hdoy8mePf8yKT98wowU_BlqvF_UO3",
     "timestamp": 1582314359164
    },
    {
     "file_id": "1soYC13f4fK8jQTXUYoKj8ZOpWu7vpVYU",
     "timestamp": 1582035152480
    },
    {
     "file_id": "19sbMl5mnIXzMvzvV1nio7abSlG5MUnil",
     "timestamp": 1581774068113
    },
    {
     "file_id": "1wES2gtJLmyTwcUFC47YbBTRpumf8hWkz",
     "timestamp": 1581425769718
    },
    {
     "file_id": "1vh9GJ2932AgfXeV0apl4uVOUWQgrvjzs",
     "timestamp": 1581339007820
    },
    {
     "file_id": "1eAwuH2cZMjDsIYziMU7gfuWUOFlhoDWp",
     "timestamp": 1581200716214
    },
    {
     "file_id": "1RYla8JWGzyh5JYOUBfRkRW4bBLDfU-CR",
     "timestamp": 1581168818609
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
